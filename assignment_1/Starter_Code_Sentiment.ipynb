{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e259e7b-d05b-41b8-b596-6ecdd4866c60",
   "metadata": {},
   "source": [
    "# DI 725: Transformers and Attention-Based Deep Networks\n",
    "\n",
    "## An Assignment for Implementing Transformers in PyTorch\n",
    "\n",
    "The purpose of this notebook is to guide you through the usage of sample code.\n",
    "\n",
    "This notebook follows the baseline prepared by Andrej Karpathy, with a custom dataset (Don-Quixote by Cervantes). This version of the code, called [nanoGPT](https://github.com/karpathy/nanoGPT), is a revisit to his famous [minGPT](https://github.com/karpathy/minGPT).\n",
    "### Author:\n",
    "* Ümit Mert Çağlar\n",
    "\n",
    "### Edited by:\n",
    "* Ebru Kültür Başaran"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715be989-8426-4406-bd8f-2bcf0e003f09",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Install requirements for your environment, comment out for later uses.\n",
    "\n",
    "Dependencies:\n",
    "\n",
    "- [pytorch](https://pytorch.org)\n",
    "- [numpy](https://numpy.org/install/)\n",
    "-  `transformers` for huggingface transformers (to load GPT-2 checkpoints)\n",
    "-  `datasets` for huggingface datasets (to download + preprocess datasets)\n",
    "-  `tiktoken` for OpenAI's fast BPE code\n",
    "-  `wandb` for optional logging\n",
    "-  `tqdm` for progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69136d40-c5ac-4623-899c-3b5ad21f368c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e72d12-9aa6-456f-ae34-2c52aaeee7c3",
   "metadata": {},
   "source": [
    "The fastest way to get started to transformers, apart from following the labs of DI725, is to use a small model and dataset. For this purpose, we will start with training a character-level GPT on the Don-Quixote by Cervantes. The code will download a single file (2MB) and apply some transformations. Examine the code [prepare.py](data/don_char/prepare.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2cade-4742-4b44-bcb2-0ae72c9571ad",
   "metadata": {},
   "source": [
    "## Preprocessing for NanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a08a93-5556-4cd9-ad2d-cc58d0363d52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 39\n",
      "Preprocessing complete! Data saved to data/sentiment\n"
     ]
    }
   ],
   "source": [
    "!python preprocess_sentiment_ng.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a34276-d844-435d-9e16-12f567969d5f",
   "metadata": {},
   "source": [
    "This script preprocesses customer conversation data for a sentiment classification task. It loads and cleans the dataset (removes URLs and punctuation, lowercases text), converts sentiment labels (like \"positive\") to numerical labels, builds a character-level vocabulary from training text, encodes the conversations as sequences of character indices and saves the encoded data and metadata (like vocab) into .pkl files for model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b258ef9-65fb-4d1f-b653-0ecc865557b8",
   "metadata": {},
   "source": [
    "## Preprocessing for GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550a7de7-960a-451e-b265-9517493fb4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_sents in main CSV: ['neutral', 'negative', 'positive']\n",
      "train_label_set: {0, 1, 2}\n",
      "val_label_set: {0, 1, 2}\n",
      "test_label_set: {0, 1, 2}\n",
      "Preprocessing complete! Data saved to data/sentiment\n"
     ]
    }
   ],
   "source": [
    "!python preprocess_sentiment_gpt2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4315ac17-4716-4c88-990d-6c73a97d1613",
   "metadata": {},
   "source": [
    "This script preprocesses a sentiment classification dataset by cleaning and encoding text conversations using the GPT-2 Byte-Pair Encoding (BPE) tokenizer from tiktoken. It splits the data into train/validation/test sets with stratification, converts labels to numeric IDs, and saves everything as pickled .pkl files for model training. It also creates and saves metadata such as the vocabulary size and label names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082ca59-adc1-4d88-8409-d3600e70337f",
   "metadata": {},
   "source": [
    "## Creating the Model Architecture for NanoGPT and GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad3097d-65d8-4a72-a8d0-b5fa463b49c8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python model_sentiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06da9f-a5a1-4621-806c-adad9b2d98d5",
   "metadata": {},
   "source": [
    "This \"model_sentiment.py\" script defines two model architectures for text classification:\n",
    "GPT2Wrapper: Fine-tunes a pre-trained GPT-2 model for a 3-class classification task (e.g., sentiment analysis).\n",
    "GPTforClassification: Placeholder for a custom GPT-style model (like NanoGPT). It's not implemented here but serves as a hook for future development.\n",
    "The GPT2Wrapper uses the average of hidden states from GPT-2 and applies a linear classifier on top. It also includes an optimizer setup with standard weight decay handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888fea7b-66fe-4784-8973-6da635f6ff52",
   "metadata": {},
   "source": [
    "## Training the Model for NanoGPT and Fine Tuning GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3380c5-976e-4dbb-b5dc-08ba56f0d93c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python train_sentiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c9439e-8905-4c41-b902-919935de68b8",
   "metadata": {},
   "source": [
    "This script fine-tunes a GPT-based model (either GPT-2 or a NanoGPT-style custom transformer) on a sentiment classification task using preprocessed conversation data. It supports distributed training (DDP) and mixed precision, uses W&B (Weights & Biases) for logging metrics and visuals, and implements early stopping based on validation loss. It loads data from pre-tokenized pickle files, trains the model with periodic evaluation, and finally logs performance (accuracy, loss, confusion matrix) on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61966bc-558b-4964-8575-1046d0aa6a91",
   "metadata": {},
   "source": [
    "**The output for NanoGPT (the script ran via Spyder)**\n",
    "\n",
    "step 0: train loss 0.7875, val loss 0.8045\n",
    "step 200: train loss 0.6565, val loss 0.7224\n",
    "step 400: train loss 0.6471, val loss 0.7015\n",
    "step 600: train loss 0.6598, val loss 0.7397\n",
    "step 800: train loss 0.6379, val loss 0.6984\n",
    "step 1000: train loss 0.6468, val loss 0.7160\n",
    "step 1200: train loss 0.6380, val loss 0.7073\n",
    "step 1400: train loss 0.6248, val loss 0.7138\n",
    "step 1600: train loss 0.6406, val loss 0.7257\n",
    "step 1800: train loss 0.6269, val loss 0.6728\n",
    "step 2000: train loss 0.6753, val loss 0.7480\n",
    "step 2200: train loss 0.6302, val loss 0.7161\n",
    "step 2400: train loss 0.6144, val loss 0.6610\n",
    "step 2600: train loss 0.6146, val loss 0.7309\n",
    "step 2800: train loss 0.5946, val loss 0.7011\n",
    "step 3000: train loss 0.5744, val loss 0.7289\n",
    "step 3200: train loss 0.5905, val loss 0.7053\n",
    "step 3400: train loss 0.5644, val loss 0.7092\n",
    "step 3600: train loss 0.5807, val loss 0.7666\n",
    "Stopping early\n",
    "Test Loss: 1.4704, Accuracy: 0.5000\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "     neutral       0.40      1.00      0.57        10\n",
    "    negative       1.00      0.50      0.67        10\n",
    "    positive       0.00      0.00      0.00        10\n",
    "\n",
    "    accuracy                           0.50        30\n",
    "   macro avg       0.47      0.50      0.41        30\n",
    "weighted avg       0.47      0.50      0.41        30\n",
    "\n",
    "**The output for GPT-2 (the script ran via Spyder):**\n",
    "\n",
    "step 0: train loss 7.0802, val loss 7.0958\n",
    "step 200: train loss 0.7538, val loss 0.7511\n",
    "step 400: train loss 0.7267, val loss 0.7346\n",
    "step 600: train loss 0.7337, val loss 0.7311\n",
    "step 800: train loss 0.7155, val loss 0.7226\n",
    "step 1000: train loss 0.7027, val loss 0.7094\n",
    "step 1200: train loss 0.6841, val loss 0.7000\n",
    "step 1400: train loss 0.6728, val loss 0.6913\n",
    "step 1600: train loss 0.6717, val loss 0.6838\n",
    "step 1800: train loss 0.6599, val loss 0.6745\n",
    "step 2000: train loss 0.6521, val loss 0.6689\n",
    "step 2200: train loss 0.6439, val loss 0.6631\n",
    "step 2400: train loss 0.6365, val loss 0.6551\n",
    "step 2600: train loss 0.6300, val loss 0.6586\n",
    "step 2800: train loss 0.6291, val loss 0.6523\n",
    "step 3000: train loss 0.6240, val loss 0.6433\n",
    "step 3200: train loss 0.6148, val loss 0.6359\n",
    "step 3400: train loss 0.6068, val loss 0.6320\n",
    "step 3600: train loss 0.5999, val loss 0.6255\n",
    "step 3800: train loss 0.5982, val loss 0.6289\n",
    "step 4000: train loss 0.5996, val loss 0.6214\n",
    "step 4200: train loss 0.5885, val loss 0.6174\n",
    "step 4400: train loss 0.5826, val loss 0.6137\n",
    "step 4600: train loss 0.5823, val loss 0.6141\n",
    "step 4800: train loss 0.5820, val loss 0.6087\n",
    "step 5000: train loss 0.5829, val loss 0.6129\n",
    "step 5200: train loss 0.5745, val loss 0.6109\n",
    "step 5400: train loss 0.5802, val loss 0.6209\n",
    "step 5600: train loss 0.5794, val loss 0.6020\n",
    "step 5800: train loss 0.5848, val loss 0.6066\n",
    "step 6000: train loss 0.5667, val loss 0.5974\n",
    "step 6200: train loss 0.5585, val loss 0.5914\n",
    "step 6400: train loss 0.5728, val loss 0.5961\n",
    "step 6600: train loss 0.5557, val loss 0.5920\n",
    "step 6800: train loss 0.5683, val loss 0.5961\n",
    "step 7000: train loss 0.5472, val loss 0.5819\n",
    "step 7200: train loss 0.5553, val loss 0.5849\n",
    "step 7400: train loss 0.5472, val loss 0.5836\n",
    "step 7600: train loss 0.5554, val loss 0.5806\n",
    "step 7800: train loss 0.5432, val loss 0.5753\n",
    "step 8000: train loss 0.5392, val loss 0.5755\n",
    "step 8200: train loss 0.5365, val loss 0.5750\n",
    "step 8400: train loss 0.5365, val loss 0.5735\n",
    "step 8600: train loss 0.5510, val loss 0.5731\n",
    "step 8800: train loss 0.5358, val loss 0.5714\n",
    "step 9000: train loss 0.5299, val loss 0.5694\n",
    "step 9200: train loss 0.5308, val loss 0.5691\n",
    "step 9400: train loss 0.5368, val loss 0.5689\n",
    "step 9600: train loss 0.5311, val loss 0.5639\n",
    "step 9800: train loss 0.5228, val loss 0.5594\n",
    "Test Loss: 2.1792, Accuracy: 0.5000\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    negative       0.83      0.50      0.62        10\n",
    "     neutral       0.42      1.00      0.59        10\n",
    "    positive       0.00      0.00      0.00        10\n",
    "\n",
    "    accuracy                           0.50        30\n",
    "   macro avg       0.42      0.50      0.40        30\n",
    "weighted avg       0.42      0.50      0.40        30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c0355-3e89-4ea0-9976-411dc15d76e5",
   "metadata": {},
   "source": [
    "## Efficiency notes\n",
    "\n",
    "*For simple model benchmarking and profiling, `bench.py` might be useful. It's identical to what happens in the meat of the training loop of `train.py`, but omits much of the other complexities.*\n",
    "\n",
    "*Note that the code by default uses [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/). At the time of writing (Dec 29, 2022) this makes `torch.compile()` available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!*\n",
    "\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "*Note that by default this repo uses PyTorch 2.0 (i.e. `torch.compile`). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you're running into related error messages try to disable this by adding `--compile=False` flag. This will slow down the code but at least it will run.*\n",
    "\n",
    "*For some context on this repository, GPT, and language modeling it might be helpful to watch [Zero To Hero series](https://karpathy.ai/zero-to-hero.html). Specifically, the [GPT video](https://www.youtube.com/watch?v=kCc8FmEb1nY) is popular if you have some prior language modeling context.*\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This code is a fork from Andrej Karpathy's introductory [NanoGPT repository](https://github.com/karpathy/nanoGPT), which is an updated form of minGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ce869-04d3-4278-a449-a0c8edb1807b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
