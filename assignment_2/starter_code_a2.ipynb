{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f98d1e-2a70-4b4e-b42f-731784709536",
   "metadata": {},
   "source": [
    "# DI 725: Transformers and Attention-Based Deep Networks\n",
    "\n",
    "## Assignment 2 : Object Detection\n",
    "\n",
    "The purpose of this notebook is to guide you through the usage of **auair_yolos.py.**\n",
    "\n",
    "### Author:\n",
    "* Ebru Kültür Başaran"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ed254-d951-4f6b-a92e-45b4b58508c2",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Install requirements for your environment, comment out for later uses.\n",
    "\n",
    "Dependencies:\n",
    "- Python >=3.8\n",
    "- pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "- pip install transformers[torch] albumentations opencv-python pycocotools torchmetrics wandb pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b08a5-3317-490c-939a-78e4d07b5780",
   "metadata": {},
   "source": [
    "## 1. Convert annotations to COCO format\n",
    "We convert to COCO format because the HuggingFace AutoImageProcessor and Trainer workflows expect COCO-style annotation JSON. Converting once at the start avoids needing custom parsing logic later and lets us leverage standardized COCO utilities such as pycocotools for data loading, augmentation, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fcca3-6e5e-4ffb-928e-d7428e925d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python auair_yolos.py convert \\\n",
    "    --ann data/AU-AIR/auair_native.json \\\n",
    "    --img-root data/AU-AIR/images \\\n",
    "    --out data/AU-AIR/auair_coco.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b015d6-ae50-425f-81b4-4140b39699fe",
   "metadata": {},
   "source": [
    "## 2. Train YOLOS-Tiny Model\n",
    "To train the model, we define the image root path, COCO converted annotations file, image size, number of epochs, number of batches, number of workers for optimum GPU usage and otput directory to save findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb7b348-de69-427b-824e-fec7cf89a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python auair_yolos.py train \\\n",
    "    --img-root data/AU-AIR/images \\\n",
    "    --ann data/AU-AIR/auair_coco.json \\\n",
    "    --img-size 384 \\\n",
    "    --epochs 5 \\\n",
    "    --batch 4 \\\n",
    "    --workers 8 \\\n",
    "    --outdir yolos-auair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02149c9a-4e91-4a05-a6ef-30d9ef2ea023",
   "metadata": {},
   "source": [
    "## 3. Evaluate the Model\n",
    "We finally evaluate our models performance on test set. The fine‑tuned model is loaded from --model-dir, then the validation dataset is built on the test samples, and the Hugging Face Trainer.evaluate method to compute per‑class AP@0.5 and overall mAP s used. Finally, the resulting metrics dictionary is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e7dbea-e100-4c1e-a27a-22799179da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python auair_yolos.py evaluate \\\n",
    "    --img-root data/AU-AIR/images \\\n",
    "    --ann data/AU-AIR/auair_coco.json \\\n",
    "    --model-dir yolos-auair \\\n",
    "    --img-size 384 \\\n",
    "    --batch 4 \\\n",
    "    --workers 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
