{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 6975,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007168458781362007,
      "grad_norm": 94.6792221069336,
      "learning_rate": 4.931192660550459e-07,
      "loss": 3.5811,
      "step": 50
    },
    {
      "epoch": 0.014336917562724014,
      "grad_norm": 111.95091247558594,
      "learning_rate": 1.06651376146789e-06,
      "loss": 3.5112,
      "step": 100
    },
    {
      "epoch": 0.021505376344086023,
      "grad_norm": 63.02651596069336,
      "learning_rate": 1.639908256880734e-06,
      "loss": 3.3545,
      "step": 150
    },
    {
      "epoch": 0.02867383512544803,
      "grad_norm": 65.95425415039062,
      "learning_rate": 2.213302752293578e-06,
      "loss": 3.0684,
      "step": 200
    },
    {
      "epoch": 0.035842293906810034,
      "grad_norm": 25.89705467224121,
      "learning_rate": 2.7866972477064223e-06,
      "loss": 2.8748,
      "step": 250
    },
    {
      "epoch": 0.043010752688172046,
      "grad_norm": 88.13428497314453,
      "learning_rate": 3.3600917431192665e-06,
      "loss": 2.5508,
      "step": 300
    },
    {
      "epoch": 0.05017921146953405,
      "grad_norm": 45.88043975830078,
      "learning_rate": 3.93348623853211e-06,
      "loss": 2.3945,
      "step": 350
    },
    {
      "epoch": 0.05734767025089606,
      "grad_norm": 75.59368133544922,
      "learning_rate": 4.5068807339449545e-06,
      "loss": 2.2547,
      "step": 400
    },
    {
      "epoch": 0.06451612903225806,
      "grad_norm": 35.772552490234375,
      "learning_rate": 5.080275229357799e-06,
      "loss": 2.138,
      "step": 450
    },
    {
      "epoch": 0.07168458781362007,
      "grad_norm": 43.26051330566406,
      "learning_rate": 5.653669724770643e-06,
      "loss": 1.9956,
      "step": 500
    },
    {
      "epoch": 0.07885304659498207,
      "grad_norm": 94.88484191894531,
      "learning_rate": 6.227064220183486e-06,
      "loss": 2.0355,
      "step": 550
    },
    {
      "epoch": 0.08602150537634409,
      "grad_norm": 91.1935043334961,
      "learning_rate": 6.8004587155963305e-06,
      "loss": 1.9752,
      "step": 600
    },
    {
      "epoch": 0.0931899641577061,
      "grad_norm": 55.290164947509766,
      "learning_rate": 7.3738532110091755e-06,
      "loss": 1.9225,
      "step": 650
    },
    {
      "epoch": 0.1003584229390681,
      "grad_norm": 40.3649787902832,
      "learning_rate": 7.947247706422018e-06,
      "loss": 1.8897,
      "step": 700
    },
    {
      "epoch": 0.10752688172043011,
      "grad_norm": 48.57997131347656,
      "learning_rate": 8.520642201834864e-06,
      "loss": 1.9114,
      "step": 750
    },
    {
      "epoch": 0.11469534050179211,
      "grad_norm": 44.484527587890625,
      "learning_rate": 9.094036697247706e-06,
      "loss": 1.8746,
      "step": 800
    },
    {
      "epoch": 0.12186379928315412,
      "grad_norm": 72.00469970703125,
      "learning_rate": 9.66743119266055e-06,
      "loss": 1.9188,
      "step": 850
    },
    {
      "epoch": 0.12903225806451613,
      "grad_norm": 85.95138549804688,
      "learning_rate": 1.0240825688073395e-05,
      "loss": 1.8603,
      "step": 900
    },
    {
      "epoch": 0.13620071684587814,
      "grad_norm": 34.68693923950195,
      "learning_rate": 1.0814220183486239e-05,
      "loss": 1.8837,
      "step": 950
    },
    {
      "epoch": 0.14336917562724014,
      "grad_norm": 116.72151184082031,
      "learning_rate": 1.1387614678899083e-05,
      "loss": 1.8065,
      "step": 1000
    },
    {
      "epoch": 0.15053763440860216,
      "grad_norm": 100.70439147949219,
      "learning_rate": 1.1961009174311929e-05,
      "loss": 1.8459,
      "step": 1050
    },
    {
      "epoch": 0.15770609318996415,
      "grad_norm": 69.32882690429688,
      "learning_rate": 1.253440366972477e-05,
      "loss": 1.712,
      "step": 1100
    },
    {
      "epoch": 0.16487455197132617,
      "grad_norm": 131.72879028320312,
      "learning_rate": 1.3107798165137616e-05,
      "loss": 1.7172,
      "step": 1150
    },
    {
      "epoch": 0.17204301075268819,
      "grad_norm": 80.95945739746094,
      "learning_rate": 1.368119266055046e-05,
      "loss": 1.7857,
      "step": 1200
    },
    {
      "epoch": 0.17921146953405018,
      "grad_norm": 85.78988647460938,
      "learning_rate": 1.4254587155963304e-05,
      "loss": 1.781,
      "step": 1250
    },
    {
      "epoch": 0.1863799283154122,
      "grad_norm": 62.13663101196289,
      "learning_rate": 1.4827981651376148e-05,
      "loss": 1.7072,
      "step": 1300
    },
    {
      "epoch": 0.1935483870967742,
      "grad_norm": 60.527957916259766,
      "learning_rate": 1.5401376146788993e-05,
      "loss": 1.8125,
      "step": 1350
    },
    {
      "epoch": 0.2007168458781362,
      "grad_norm": 39.60439682006836,
      "learning_rate": 1.5974770642201837e-05,
      "loss": 1.6855,
      "step": 1400
    },
    {
      "epoch": 0.2078853046594982,
      "grad_norm": 119.29389190673828,
      "learning_rate": 1.654816513761468e-05,
      "loss": 1.7185,
      "step": 1450
    },
    {
      "epoch": 0.21505376344086022,
      "grad_norm": 48.4014778137207,
      "learning_rate": 1.7121559633027525e-05,
      "loss": 1.8392,
      "step": 1500
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 74.41249084472656,
      "learning_rate": 1.769495412844037e-05,
      "loss": 1.7089,
      "step": 1550
    },
    {
      "epoch": 0.22939068100358423,
      "grad_norm": 80.64263916015625,
      "learning_rate": 1.826834862385321e-05,
      "loss": 1.7053,
      "step": 1600
    },
    {
      "epoch": 0.23655913978494625,
      "grad_norm": 75.54520416259766,
      "learning_rate": 1.8841743119266055e-05,
      "loss": 1.7721,
      "step": 1650
    },
    {
      "epoch": 0.24372759856630824,
      "grad_norm": 95.68910217285156,
      "learning_rate": 1.9415137614678902e-05,
      "loss": 1.8003,
      "step": 1700
    },
    {
      "epoch": 0.25089605734767023,
      "grad_norm": 88.0743637084961,
      "learning_rate": 1.9988532110091746e-05,
      "loss": 1.7913,
      "step": 1750
    },
    {
      "epoch": 0.25806451612903225,
      "grad_norm": 84.54595947265625,
      "learning_rate": 1.997042045214452e-05,
      "loss": 1.7547,
      "step": 1800
    },
    {
      "epoch": 0.26523297491039427,
      "grad_norm": 62.25143051147461,
      "learning_rate": 1.9940237240047088e-05,
      "loss": 1.7842,
      "step": 1850
    },
    {
      "epoch": 0.2724014336917563,
      "grad_norm": 132.93038940429688,
      "learning_rate": 1.9910054027949656e-05,
      "loss": 1.7194,
      "step": 1900
    },
    {
      "epoch": 0.27956989247311825,
      "grad_norm": 114.3082046508789,
      "learning_rate": 1.9879870815852225e-05,
      "loss": 1.7241,
      "step": 1950
    },
    {
      "epoch": 0.2867383512544803,
      "grad_norm": 78.41365051269531,
      "learning_rate": 1.9849687603754794e-05,
      "loss": 1.8103,
      "step": 2000
    },
    {
      "epoch": 0.2939068100358423,
      "grad_norm": 111.15614318847656,
      "learning_rate": 1.9819504391657362e-05,
      "loss": 1.722,
      "step": 2050
    },
    {
      "epoch": 0.3010752688172043,
      "grad_norm": 64.8713150024414,
      "learning_rate": 1.978932117955993e-05,
      "loss": 1.7172,
      "step": 2100
    },
    {
      "epoch": 0.30824372759856633,
      "grad_norm": 52.56601333618164,
      "learning_rate": 1.97591379674625e-05,
      "loss": 1.748,
      "step": 2150
    },
    {
      "epoch": 0.3154121863799283,
      "grad_norm": 48.4078369140625,
      "learning_rate": 1.9729558419607017e-05,
      "loss": 1.6385,
      "step": 2200
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 49.44916915893555,
      "learning_rate": 1.9699375207509585e-05,
      "loss": 1.7564,
      "step": 2250
    },
    {
      "epoch": 0.32974910394265233,
      "grad_norm": 78.5113525390625,
      "learning_rate": 1.9669191995412154e-05,
      "loss": 1.8061,
      "step": 2300
    },
    {
      "epoch": 0.33691756272401435,
      "grad_norm": 89.51522827148438,
      "learning_rate": 1.9639008783314722e-05,
      "loss": 1.7959,
      "step": 2350
    },
    {
      "epoch": 0.34408602150537637,
      "grad_norm": 77.31348419189453,
      "learning_rate": 1.960882557121729e-05,
      "loss": 1.7281,
      "step": 2400
    },
    {
      "epoch": 0.35125448028673834,
      "grad_norm": 35.14601135253906,
      "learning_rate": 1.957864235911986e-05,
      "loss": 1.7025,
      "step": 2450
    },
    {
      "epoch": 0.35842293906810035,
      "grad_norm": 170.42095947265625,
      "learning_rate": 1.9548459147022428e-05,
      "loss": 1.6293,
      "step": 2500
    },
    {
      "epoch": 0.3655913978494624,
      "grad_norm": 115.96540069580078,
      "learning_rate": 1.9518275934924997e-05,
      "loss": 1.6109,
      "step": 2550
    },
    {
      "epoch": 0.3727598566308244,
      "grad_norm": 78.90654754638672,
      "learning_rate": 1.9488092722827565e-05,
      "loss": 1.6356,
      "step": 2600
    },
    {
      "epoch": 0.37992831541218636,
      "grad_norm": 91.19440460205078,
      "learning_rate": 1.9457909510730134e-05,
      "loss": 1.665,
      "step": 2650
    },
    {
      "epoch": 0.3870967741935484,
      "grad_norm": 67.31293487548828,
      "learning_rate": 1.9427726298632702e-05,
      "loss": 1.6013,
      "step": 2700
    },
    {
      "epoch": 0.3942652329749104,
      "grad_norm": 35.623600006103516,
      "learning_rate": 1.939754308653527e-05,
      "loss": 1.7006,
      "step": 2750
    },
    {
      "epoch": 0.4014336917562724,
      "grad_norm": 57.7923469543457,
      "learning_rate": 1.936735987443784e-05,
      "loss": 1.595,
      "step": 2800
    },
    {
      "epoch": 0.40860215053763443,
      "grad_norm": 53.78890609741211,
      "learning_rate": 1.9337176662340408e-05,
      "loss": 1.6378,
      "step": 2850
    },
    {
      "epoch": 0.4157706093189964,
      "grad_norm": 31.879650115966797,
      "learning_rate": 1.9306993450242976e-05,
      "loss": 1.6242,
      "step": 2900
    },
    {
      "epoch": 0.4229390681003584,
      "grad_norm": 41.607337951660156,
      "learning_rate": 1.9276810238145545e-05,
      "loss": 1.7083,
      "step": 2950
    },
    {
      "epoch": 0.43010752688172044,
      "grad_norm": 75.56098937988281,
      "learning_rate": 1.9246627026048113e-05,
      "loss": 1.6862,
      "step": 3000
    },
    {
      "epoch": 0.43727598566308246,
      "grad_norm": 66.35958099365234,
      "learning_rate": 1.9216443813950682e-05,
      "loss": 1.638,
      "step": 3050
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 52.193912506103516,
      "learning_rate": 1.918626060185325e-05,
      "loss": 1.6719,
      "step": 3100
    },
    {
      "epoch": 0.45161290322580644,
      "grad_norm": 66.34355163574219,
      "learning_rate": 1.915607738975582e-05,
      "loss": 1.6849,
      "step": 3150
    },
    {
      "epoch": 0.45878136200716846,
      "grad_norm": 52.57299041748047,
      "learning_rate": 1.9125894177658387e-05,
      "loss": 1.7525,
      "step": 3200
    },
    {
      "epoch": 0.4659498207885305,
      "grad_norm": 59.958160400390625,
      "learning_rate": 1.9095710965560956e-05,
      "loss": 1.7571,
      "step": 3250
    },
    {
      "epoch": 0.4731182795698925,
      "grad_norm": 53.841007232666016,
      "learning_rate": 1.9065527753463524e-05,
      "loss": 1.5363,
      "step": 3300
    },
    {
      "epoch": 0.48028673835125446,
      "grad_norm": 49.62135696411133,
      "learning_rate": 1.9035344541366093e-05,
      "loss": 1.5725,
      "step": 3350
    },
    {
      "epoch": 0.4874551971326165,
      "grad_norm": 31.244384765625,
      "learning_rate": 1.900516132926866e-05,
      "loss": 1.7516,
      "step": 3400
    },
    {
      "epoch": 0.4946236559139785,
      "grad_norm": 77.0704574584961,
      "learning_rate": 1.897497811717123e-05,
      "loss": 1.4213,
      "step": 3450
    },
    {
      "epoch": 0.5017921146953405,
      "grad_norm": 80.37962341308594,
      "learning_rate": 1.89447949050738e-05,
      "loss": 1.713,
      "step": 3500
    },
    {
      "epoch": 0.5089605734767025,
      "grad_norm": 164.86434936523438,
      "learning_rate": 1.8914611692976367e-05,
      "loss": 1.6249,
      "step": 3550
    },
    {
      "epoch": 0.5161290322580645,
      "grad_norm": 57.523372650146484,
      "learning_rate": 1.8884428480878936e-05,
      "loss": 1.6016,
      "step": 3600
    },
    {
      "epoch": 0.5232974910394266,
      "grad_norm": 50.754878997802734,
      "learning_rate": 1.8854245268781507e-05,
      "loss": 1.6603,
      "step": 3650
    },
    {
      "epoch": 0.5304659498207885,
      "grad_norm": 55.45808029174805,
      "learning_rate": 1.8824062056684073e-05,
      "loss": 1.7404,
      "step": 3700
    },
    {
      "epoch": 0.5376344086021505,
      "grad_norm": 37.93119430541992,
      "learning_rate": 1.879387884458664e-05,
      "loss": 1.6315,
      "step": 3750
    },
    {
      "epoch": 0.5448028673835126,
      "grad_norm": 135.80096435546875,
      "learning_rate": 1.8763695632489213e-05,
      "loss": 1.5807,
      "step": 3800
    },
    {
      "epoch": 0.5519713261648745,
      "grad_norm": 59.26798629760742,
      "learning_rate": 1.8733512420391778e-05,
      "loss": 1.6142,
      "step": 3850
    },
    {
      "epoch": 0.5591397849462365,
      "grad_norm": 96.2778091430664,
      "learning_rate": 1.8703329208294347e-05,
      "loss": 1.6471,
      "step": 3900
    },
    {
      "epoch": 0.5663082437275986,
      "grad_norm": 57.643821716308594,
      "learning_rate": 1.867314599619692e-05,
      "loss": 1.5491,
      "step": 3950
    },
    {
      "epoch": 0.5734767025089605,
      "grad_norm": 35.75929641723633,
      "learning_rate": 1.8642962784099484e-05,
      "loss": 1.6596,
      "step": 4000
    },
    {
      "epoch": 0.5806451612903226,
      "grad_norm": 36.89448928833008,
      "learning_rate": 1.8612779572002052e-05,
      "loss": 1.6438,
      "step": 4050
    },
    {
      "epoch": 0.5878136200716846,
      "grad_norm": 80.80117797851562,
      "learning_rate": 1.8582596359904624e-05,
      "loss": 1.7531,
      "step": 4100
    },
    {
      "epoch": 0.5949820788530465,
      "grad_norm": 78.25704193115234,
      "learning_rate": 1.8552413147807193e-05,
      "loss": 1.6402,
      "step": 4150
    },
    {
      "epoch": 0.6021505376344086,
      "grad_norm": 26.585716247558594,
      "learning_rate": 1.8522229935709758e-05,
      "loss": 1.5913,
      "step": 4200
    },
    {
      "epoch": 0.6093189964157706,
      "grad_norm": 86.67760467529297,
      "learning_rate": 1.849204672361233e-05,
      "loss": 1.6143,
      "step": 4250
    },
    {
      "epoch": 0.6164874551971327,
      "grad_norm": 88.61467742919922,
      "learning_rate": 1.8461863511514898e-05,
      "loss": 1.5779,
      "step": 4300
    },
    {
      "epoch": 0.6236559139784946,
      "grad_norm": 108.47418975830078,
      "learning_rate": 1.8432283963659413e-05,
      "loss": 1.6308,
      "step": 4350
    },
    {
      "epoch": 0.6308243727598566,
      "grad_norm": 66.7287368774414,
      "learning_rate": 1.840210075156198e-05,
      "loss": 1.7253,
      "step": 4400
    },
    {
      "epoch": 0.6379928315412187,
      "grad_norm": 87.00125122070312,
      "learning_rate": 1.8371917539464553e-05,
      "loss": 1.5943,
      "step": 4450
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 66.71989440917969,
      "learning_rate": 1.834173432736712e-05,
      "loss": 1.4956,
      "step": 4500
    },
    {
      "epoch": 0.6523297491039427,
      "grad_norm": 31.883338928222656,
      "learning_rate": 1.8311551115269687e-05,
      "loss": 1.5243,
      "step": 4550
    },
    {
      "epoch": 0.6594982078853047,
      "grad_norm": 51.31227111816406,
      "learning_rate": 1.828136790317226e-05,
      "loss": 1.5003,
      "step": 4600
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 38.90046691894531,
      "learning_rate": 1.8251184691074827e-05,
      "loss": 1.617,
      "step": 4650
    },
    {
      "epoch": 0.6738351254480287,
      "grad_norm": 103.26469421386719,
      "learning_rate": 1.8221001478977392e-05,
      "loss": 1.6499,
      "step": 4700
    },
    {
      "epoch": 0.6810035842293907,
      "grad_norm": 63.37947082519531,
      "learning_rate": 1.8190818266879964e-05,
      "loss": 1.584,
      "step": 4750
    },
    {
      "epoch": 0.6881720430107527,
      "grad_norm": 74.27564239501953,
      "learning_rate": 1.8160635054782533e-05,
      "loss": 1.5422,
      "step": 4800
    },
    {
      "epoch": 0.6953405017921147,
      "grad_norm": 99.98155212402344,
      "learning_rate": 1.8130451842685098e-05,
      "loss": 1.6406,
      "step": 4850
    },
    {
      "epoch": 0.7025089605734767,
      "grad_norm": 53.560604095458984,
      "learning_rate": 1.810026863058767e-05,
      "loss": 1.5312,
      "step": 4900
    },
    {
      "epoch": 0.7096774193548387,
      "grad_norm": 71.31781005859375,
      "learning_rate": 1.807008541849024e-05,
      "loss": 1.5775,
      "step": 4950
    },
    {
      "epoch": 0.7168458781362007,
      "grad_norm": 34.07607650756836,
      "learning_rate": 1.8039902206392807e-05,
      "loss": 1.6422,
      "step": 5000
    },
    {
      "epoch": 0.7240143369175627,
      "grad_norm": 51.42107391357422,
      "learning_rate": 1.8009718994295375e-05,
      "loss": 1.5511,
      "step": 5050
    },
    {
      "epoch": 0.7311827956989247,
      "grad_norm": 77.08232116699219,
      "learning_rate": 1.7979535782197944e-05,
      "loss": 1.6014,
      "step": 5100
    },
    {
      "epoch": 0.7383512544802867,
      "grad_norm": 102.47296142578125,
      "learning_rate": 1.7949352570100512e-05,
      "loss": 1.5089,
      "step": 5150
    },
    {
      "epoch": 0.7455197132616488,
      "grad_norm": 49.910221099853516,
      "learning_rate": 1.791916935800308e-05,
      "loss": 1.5552,
      "step": 5200
    },
    {
      "epoch": 0.7526881720430108,
      "grad_norm": 88.1629867553711,
      "learning_rate": 1.788898614590565e-05,
      "loss": 1.6657,
      "step": 5250
    },
    {
      "epoch": 0.7598566308243727,
      "grad_norm": 143.9617462158203,
      "learning_rate": 1.7858802933808218e-05,
      "loss": 1.6115,
      "step": 5300
    },
    {
      "epoch": 0.7670250896057348,
      "grad_norm": 88.4598388671875,
      "learning_rate": 1.7828619721710783e-05,
      "loss": 1.5889,
      "step": 5350
    },
    {
      "epoch": 0.7741935483870968,
      "grad_norm": 133.00912475585938,
      "learning_rate": 1.7798436509613355e-05,
      "loss": 1.539,
      "step": 5400
    },
    {
      "epoch": 0.7813620071684588,
      "grad_norm": 123.47762298583984,
      "learning_rate": 1.7768253297515924e-05,
      "loss": 1.464,
      "step": 5450
    },
    {
      "epoch": 0.7885304659498208,
      "grad_norm": 88.6274642944336,
      "learning_rate": 1.7738070085418492e-05,
      "loss": 1.5803,
      "step": 5500
    },
    {
      "epoch": 0.7956989247311828,
      "grad_norm": 85.1960220336914,
      "learning_rate": 1.770788687332106e-05,
      "loss": 1.6619,
      "step": 5550
    },
    {
      "epoch": 0.8028673835125448,
      "grad_norm": 67.58277893066406,
      "learning_rate": 1.767770366122363e-05,
      "loss": 1.4546,
      "step": 5600
    },
    {
      "epoch": 0.8100358422939068,
      "grad_norm": 96.83509063720703,
      "learning_rate": 1.7647520449126198e-05,
      "loss": 1.5103,
      "step": 5650
    },
    {
      "epoch": 0.8172043010752689,
      "grad_norm": 52.09833526611328,
      "learning_rate": 1.7617337237028766e-05,
      "loss": 1.6581,
      "step": 5700
    },
    {
      "epoch": 0.8243727598566308,
      "grad_norm": 68.51612091064453,
      "learning_rate": 1.7587154024931335e-05,
      "loss": 1.5091,
      "step": 5750
    },
    {
      "epoch": 0.8315412186379928,
      "grad_norm": 43.92729568481445,
      "learning_rate": 1.7556970812833903e-05,
      "loss": 1.5616,
      "step": 5800
    },
    {
      "epoch": 0.8387096774193549,
      "grad_norm": 50.5400505065918,
      "learning_rate": 1.7526787600736472e-05,
      "loss": 1.417,
      "step": 5850
    },
    {
      "epoch": 0.8458781362007168,
      "grad_norm": 41.58968734741211,
      "learning_rate": 1.749660438863904e-05,
      "loss": 1.6076,
      "step": 5900
    },
    {
      "epoch": 0.8530465949820788,
      "grad_norm": 134.29019165039062,
      "learning_rate": 1.746642117654161e-05,
      "loss": 1.5963,
      "step": 5950
    },
    {
      "epoch": 0.8602150537634409,
      "grad_norm": 85.39311218261719,
      "learning_rate": 1.7436237964444177e-05,
      "loss": 1.5891,
      "step": 6000
    },
    {
      "epoch": 0.8673835125448028,
      "grad_norm": 68.91819763183594,
      "learning_rate": 1.7406054752346746e-05,
      "loss": 1.5933,
      "step": 6050
    },
    {
      "epoch": 0.8745519713261649,
      "grad_norm": 136.87306213378906,
      "learning_rate": 1.7375871540249314e-05,
      "loss": 1.5421,
      "step": 6100
    },
    {
      "epoch": 0.8817204301075269,
      "grad_norm": 44.858192443847656,
      "learning_rate": 1.7345688328151883e-05,
      "loss": 1.4493,
      "step": 6150
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 76.70758819580078,
      "learning_rate": 1.731550511605445e-05,
      "loss": 1.5475,
      "step": 6200
    },
    {
      "epoch": 0.8960573476702509,
      "grad_norm": 150.49496459960938,
      "learning_rate": 1.728532190395702e-05,
      "loss": 1.5496,
      "step": 6250
    },
    {
      "epoch": 0.9032258064516129,
      "grad_norm": 116.65199279785156,
      "learning_rate": 1.725513869185959e-05,
      "loss": 1.4916,
      "step": 6300
    },
    {
      "epoch": 0.910394265232975,
      "grad_norm": 104.24698638916016,
      "learning_rate": 1.7224955479762157e-05,
      "loss": 1.571,
      "step": 6350
    },
    {
      "epoch": 0.9175627240143369,
      "grad_norm": 78.9054183959961,
      "learning_rate": 1.7195375931906675e-05,
      "loss": 1.5284,
      "step": 6400
    },
    {
      "epoch": 0.9247311827956989,
      "grad_norm": 106.78584289550781,
      "learning_rate": 1.7165192719809243e-05,
      "loss": 1.5845,
      "step": 6450
    },
    {
      "epoch": 0.931899641577061,
      "grad_norm": 60.75614547729492,
      "learning_rate": 1.7135009507711812e-05,
      "loss": 1.5817,
      "step": 6500
    },
    {
      "epoch": 0.9390681003584229,
      "grad_norm": 47.65946578979492,
      "learning_rate": 1.710482629561438e-05,
      "loss": 1.6222,
      "step": 6550
    },
    {
      "epoch": 0.946236559139785,
      "grad_norm": 48.58771896362305,
      "learning_rate": 1.707464308351695e-05,
      "loss": 1.5109,
      "step": 6600
    },
    {
      "epoch": 0.953405017921147,
      "grad_norm": 118.88809967041016,
      "learning_rate": 1.7044459871419517e-05,
      "loss": 1.4388,
      "step": 6650
    },
    {
      "epoch": 0.9605734767025089,
      "grad_norm": 39.9713249206543,
      "learning_rate": 1.7014276659322086e-05,
      "loss": 1.5587,
      "step": 6700
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 88.2680435180664,
      "learning_rate": 1.6984093447224655e-05,
      "loss": 1.6799,
      "step": 6750
    },
    {
      "epoch": 0.974910394265233,
      "grad_norm": 130.3756561279297,
      "learning_rate": 1.6953910235127223e-05,
      "loss": 1.5514,
      "step": 6800
    },
    {
      "epoch": 0.982078853046595,
      "grad_norm": 65.55506134033203,
      "learning_rate": 1.692372702302979e-05,
      "loss": 1.4668,
      "step": 6850
    },
    {
      "epoch": 0.989247311827957,
      "grad_norm": 59.22216033935547,
      "learning_rate": 1.689354381093236e-05,
      "loss": 1.5667,
      "step": 6900
    },
    {
      "epoch": 0.996415770609319,
      "grad_norm": 46.19839859008789,
      "learning_rate": 1.686336059883493e-05,
      "loss": 1.5624,
      "step": 6950
    }
  ],
  "logging_steps": 50,
  "max_steps": 34875,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.520943451488911e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
