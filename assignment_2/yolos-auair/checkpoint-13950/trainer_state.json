{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 13950,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007168458781362007,
      "grad_norm": 94.6792221069336,
      "learning_rate": 4.931192660550459e-07,
      "loss": 3.5811,
      "step": 50
    },
    {
      "epoch": 0.014336917562724014,
      "grad_norm": 111.95091247558594,
      "learning_rate": 1.06651376146789e-06,
      "loss": 3.5112,
      "step": 100
    },
    {
      "epoch": 0.021505376344086023,
      "grad_norm": 63.02651596069336,
      "learning_rate": 1.639908256880734e-06,
      "loss": 3.3545,
      "step": 150
    },
    {
      "epoch": 0.02867383512544803,
      "grad_norm": 65.95425415039062,
      "learning_rate": 2.213302752293578e-06,
      "loss": 3.0684,
      "step": 200
    },
    {
      "epoch": 0.035842293906810034,
      "grad_norm": 25.89705467224121,
      "learning_rate": 2.7866972477064223e-06,
      "loss": 2.8748,
      "step": 250
    },
    {
      "epoch": 0.043010752688172046,
      "grad_norm": 88.13428497314453,
      "learning_rate": 3.3600917431192665e-06,
      "loss": 2.5508,
      "step": 300
    },
    {
      "epoch": 0.05017921146953405,
      "grad_norm": 45.88043975830078,
      "learning_rate": 3.93348623853211e-06,
      "loss": 2.3945,
      "step": 350
    },
    {
      "epoch": 0.05734767025089606,
      "grad_norm": 75.59368133544922,
      "learning_rate": 4.5068807339449545e-06,
      "loss": 2.2547,
      "step": 400
    },
    {
      "epoch": 0.06451612903225806,
      "grad_norm": 35.772552490234375,
      "learning_rate": 5.080275229357799e-06,
      "loss": 2.138,
      "step": 450
    },
    {
      "epoch": 0.07168458781362007,
      "grad_norm": 43.26051330566406,
      "learning_rate": 5.653669724770643e-06,
      "loss": 1.9956,
      "step": 500
    },
    {
      "epoch": 0.07885304659498207,
      "grad_norm": 94.88484191894531,
      "learning_rate": 6.227064220183486e-06,
      "loss": 2.0355,
      "step": 550
    },
    {
      "epoch": 0.08602150537634409,
      "grad_norm": 91.1935043334961,
      "learning_rate": 6.8004587155963305e-06,
      "loss": 1.9752,
      "step": 600
    },
    {
      "epoch": 0.0931899641577061,
      "grad_norm": 55.290164947509766,
      "learning_rate": 7.3738532110091755e-06,
      "loss": 1.9225,
      "step": 650
    },
    {
      "epoch": 0.1003584229390681,
      "grad_norm": 40.3649787902832,
      "learning_rate": 7.947247706422018e-06,
      "loss": 1.8897,
      "step": 700
    },
    {
      "epoch": 0.10752688172043011,
      "grad_norm": 48.57997131347656,
      "learning_rate": 8.520642201834864e-06,
      "loss": 1.9114,
      "step": 750
    },
    {
      "epoch": 0.11469534050179211,
      "grad_norm": 44.484527587890625,
      "learning_rate": 9.094036697247706e-06,
      "loss": 1.8746,
      "step": 800
    },
    {
      "epoch": 0.12186379928315412,
      "grad_norm": 72.00469970703125,
      "learning_rate": 9.66743119266055e-06,
      "loss": 1.9188,
      "step": 850
    },
    {
      "epoch": 0.12903225806451613,
      "grad_norm": 85.95138549804688,
      "learning_rate": 1.0240825688073395e-05,
      "loss": 1.8603,
      "step": 900
    },
    {
      "epoch": 0.13620071684587814,
      "grad_norm": 34.68693923950195,
      "learning_rate": 1.0814220183486239e-05,
      "loss": 1.8837,
      "step": 950
    },
    {
      "epoch": 0.14336917562724014,
      "grad_norm": 116.72151184082031,
      "learning_rate": 1.1387614678899083e-05,
      "loss": 1.8065,
      "step": 1000
    },
    {
      "epoch": 0.15053763440860216,
      "grad_norm": 100.70439147949219,
      "learning_rate": 1.1961009174311929e-05,
      "loss": 1.8459,
      "step": 1050
    },
    {
      "epoch": 0.15770609318996415,
      "grad_norm": 69.32882690429688,
      "learning_rate": 1.253440366972477e-05,
      "loss": 1.712,
      "step": 1100
    },
    {
      "epoch": 0.16487455197132617,
      "grad_norm": 131.72879028320312,
      "learning_rate": 1.3107798165137616e-05,
      "loss": 1.7172,
      "step": 1150
    },
    {
      "epoch": 0.17204301075268819,
      "grad_norm": 80.95945739746094,
      "learning_rate": 1.368119266055046e-05,
      "loss": 1.7857,
      "step": 1200
    },
    {
      "epoch": 0.17921146953405018,
      "grad_norm": 85.78988647460938,
      "learning_rate": 1.4254587155963304e-05,
      "loss": 1.781,
      "step": 1250
    },
    {
      "epoch": 0.1863799283154122,
      "grad_norm": 62.13663101196289,
      "learning_rate": 1.4827981651376148e-05,
      "loss": 1.7072,
      "step": 1300
    },
    {
      "epoch": 0.1935483870967742,
      "grad_norm": 60.527957916259766,
      "learning_rate": 1.5401376146788993e-05,
      "loss": 1.8125,
      "step": 1350
    },
    {
      "epoch": 0.2007168458781362,
      "grad_norm": 39.60439682006836,
      "learning_rate": 1.5974770642201837e-05,
      "loss": 1.6855,
      "step": 1400
    },
    {
      "epoch": 0.2078853046594982,
      "grad_norm": 119.29389190673828,
      "learning_rate": 1.654816513761468e-05,
      "loss": 1.7185,
      "step": 1450
    },
    {
      "epoch": 0.21505376344086022,
      "grad_norm": 48.4014778137207,
      "learning_rate": 1.7121559633027525e-05,
      "loss": 1.8392,
      "step": 1500
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 74.41249084472656,
      "learning_rate": 1.769495412844037e-05,
      "loss": 1.7089,
      "step": 1550
    },
    {
      "epoch": 0.22939068100358423,
      "grad_norm": 80.64263916015625,
      "learning_rate": 1.826834862385321e-05,
      "loss": 1.7053,
      "step": 1600
    },
    {
      "epoch": 0.23655913978494625,
      "grad_norm": 75.54520416259766,
      "learning_rate": 1.8841743119266055e-05,
      "loss": 1.7721,
      "step": 1650
    },
    {
      "epoch": 0.24372759856630824,
      "grad_norm": 95.68910217285156,
      "learning_rate": 1.9415137614678902e-05,
      "loss": 1.8003,
      "step": 1700
    },
    {
      "epoch": 0.25089605734767023,
      "grad_norm": 88.0743637084961,
      "learning_rate": 1.9988532110091746e-05,
      "loss": 1.7913,
      "step": 1750
    },
    {
      "epoch": 0.25806451612903225,
      "grad_norm": 84.54595947265625,
      "learning_rate": 1.997042045214452e-05,
      "loss": 1.7547,
      "step": 1800
    },
    {
      "epoch": 0.26523297491039427,
      "grad_norm": 62.25143051147461,
      "learning_rate": 1.9940237240047088e-05,
      "loss": 1.7842,
      "step": 1850
    },
    {
      "epoch": 0.2724014336917563,
      "grad_norm": 132.93038940429688,
      "learning_rate": 1.9910054027949656e-05,
      "loss": 1.7194,
      "step": 1900
    },
    {
      "epoch": 0.27956989247311825,
      "grad_norm": 114.3082046508789,
      "learning_rate": 1.9879870815852225e-05,
      "loss": 1.7241,
      "step": 1950
    },
    {
      "epoch": 0.2867383512544803,
      "grad_norm": 78.41365051269531,
      "learning_rate": 1.9849687603754794e-05,
      "loss": 1.8103,
      "step": 2000
    },
    {
      "epoch": 0.2939068100358423,
      "grad_norm": 111.15614318847656,
      "learning_rate": 1.9819504391657362e-05,
      "loss": 1.722,
      "step": 2050
    },
    {
      "epoch": 0.3010752688172043,
      "grad_norm": 64.8713150024414,
      "learning_rate": 1.978932117955993e-05,
      "loss": 1.7172,
      "step": 2100
    },
    {
      "epoch": 0.30824372759856633,
      "grad_norm": 52.56601333618164,
      "learning_rate": 1.97591379674625e-05,
      "loss": 1.748,
      "step": 2150
    },
    {
      "epoch": 0.3154121863799283,
      "grad_norm": 48.4078369140625,
      "learning_rate": 1.9729558419607017e-05,
      "loss": 1.6385,
      "step": 2200
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 49.44916915893555,
      "learning_rate": 1.9699375207509585e-05,
      "loss": 1.7564,
      "step": 2250
    },
    {
      "epoch": 0.32974910394265233,
      "grad_norm": 78.5113525390625,
      "learning_rate": 1.9669191995412154e-05,
      "loss": 1.8061,
      "step": 2300
    },
    {
      "epoch": 0.33691756272401435,
      "grad_norm": 89.51522827148438,
      "learning_rate": 1.9639008783314722e-05,
      "loss": 1.7959,
      "step": 2350
    },
    {
      "epoch": 0.34408602150537637,
      "grad_norm": 77.31348419189453,
      "learning_rate": 1.960882557121729e-05,
      "loss": 1.7281,
      "step": 2400
    },
    {
      "epoch": 0.35125448028673834,
      "grad_norm": 35.14601135253906,
      "learning_rate": 1.957864235911986e-05,
      "loss": 1.7025,
      "step": 2450
    },
    {
      "epoch": 0.35842293906810035,
      "grad_norm": 170.42095947265625,
      "learning_rate": 1.9548459147022428e-05,
      "loss": 1.6293,
      "step": 2500
    },
    {
      "epoch": 0.3655913978494624,
      "grad_norm": 115.96540069580078,
      "learning_rate": 1.9518275934924997e-05,
      "loss": 1.6109,
      "step": 2550
    },
    {
      "epoch": 0.3727598566308244,
      "grad_norm": 78.90654754638672,
      "learning_rate": 1.9488092722827565e-05,
      "loss": 1.6356,
      "step": 2600
    },
    {
      "epoch": 0.37992831541218636,
      "grad_norm": 91.19440460205078,
      "learning_rate": 1.9457909510730134e-05,
      "loss": 1.665,
      "step": 2650
    },
    {
      "epoch": 0.3870967741935484,
      "grad_norm": 67.31293487548828,
      "learning_rate": 1.9427726298632702e-05,
      "loss": 1.6013,
      "step": 2700
    },
    {
      "epoch": 0.3942652329749104,
      "grad_norm": 35.623600006103516,
      "learning_rate": 1.939754308653527e-05,
      "loss": 1.7006,
      "step": 2750
    },
    {
      "epoch": 0.4014336917562724,
      "grad_norm": 57.7923469543457,
      "learning_rate": 1.936735987443784e-05,
      "loss": 1.595,
      "step": 2800
    },
    {
      "epoch": 0.40860215053763443,
      "grad_norm": 53.78890609741211,
      "learning_rate": 1.9337176662340408e-05,
      "loss": 1.6378,
      "step": 2850
    },
    {
      "epoch": 0.4157706093189964,
      "grad_norm": 31.879650115966797,
      "learning_rate": 1.9306993450242976e-05,
      "loss": 1.6242,
      "step": 2900
    },
    {
      "epoch": 0.4229390681003584,
      "grad_norm": 41.607337951660156,
      "learning_rate": 1.9276810238145545e-05,
      "loss": 1.7083,
      "step": 2950
    },
    {
      "epoch": 0.43010752688172044,
      "grad_norm": 75.56098937988281,
      "learning_rate": 1.9246627026048113e-05,
      "loss": 1.6862,
      "step": 3000
    },
    {
      "epoch": 0.43727598566308246,
      "grad_norm": 66.35958099365234,
      "learning_rate": 1.9216443813950682e-05,
      "loss": 1.638,
      "step": 3050
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 52.193912506103516,
      "learning_rate": 1.918626060185325e-05,
      "loss": 1.6719,
      "step": 3100
    },
    {
      "epoch": 0.45161290322580644,
      "grad_norm": 66.34355163574219,
      "learning_rate": 1.915607738975582e-05,
      "loss": 1.6849,
      "step": 3150
    },
    {
      "epoch": 0.45878136200716846,
      "grad_norm": 52.57299041748047,
      "learning_rate": 1.9125894177658387e-05,
      "loss": 1.7525,
      "step": 3200
    },
    {
      "epoch": 0.4659498207885305,
      "grad_norm": 59.958160400390625,
      "learning_rate": 1.9095710965560956e-05,
      "loss": 1.7571,
      "step": 3250
    },
    {
      "epoch": 0.4731182795698925,
      "grad_norm": 53.841007232666016,
      "learning_rate": 1.9065527753463524e-05,
      "loss": 1.5363,
      "step": 3300
    },
    {
      "epoch": 0.48028673835125446,
      "grad_norm": 49.62135696411133,
      "learning_rate": 1.9035344541366093e-05,
      "loss": 1.5725,
      "step": 3350
    },
    {
      "epoch": 0.4874551971326165,
      "grad_norm": 31.244384765625,
      "learning_rate": 1.900516132926866e-05,
      "loss": 1.7516,
      "step": 3400
    },
    {
      "epoch": 0.4946236559139785,
      "grad_norm": 77.0704574584961,
      "learning_rate": 1.897497811717123e-05,
      "loss": 1.4213,
      "step": 3450
    },
    {
      "epoch": 0.5017921146953405,
      "grad_norm": 80.37962341308594,
      "learning_rate": 1.89447949050738e-05,
      "loss": 1.713,
      "step": 3500
    },
    {
      "epoch": 0.5089605734767025,
      "grad_norm": 164.86434936523438,
      "learning_rate": 1.8914611692976367e-05,
      "loss": 1.6249,
      "step": 3550
    },
    {
      "epoch": 0.5161290322580645,
      "grad_norm": 57.523372650146484,
      "learning_rate": 1.8884428480878936e-05,
      "loss": 1.6016,
      "step": 3600
    },
    {
      "epoch": 0.5232974910394266,
      "grad_norm": 50.754878997802734,
      "learning_rate": 1.8854245268781507e-05,
      "loss": 1.6603,
      "step": 3650
    },
    {
      "epoch": 0.5304659498207885,
      "grad_norm": 55.45808029174805,
      "learning_rate": 1.8824062056684073e-05,
      "loss": 1.7404,
      "step": 3700
    },
    {
      "epoch": 0.5376344086021505,
      "grad_norm": 37.93119430541992,
      "learning_rate": 1.879387884458664e-05,
      "loss": 1.6315,
      "step": 3750
    },
    {
      "epoch": 0.5448028673835126,
      "grad_norm": 135.80096435546875,
      "learning_rate": 1.8763695632489213e-05,
      "loss": 1.5807,
      "step": 3800
    },
    {
      "epoch": 0.5519713261648745,
      "grad_norm": 59.26798629760742,
      "learning_rate": 1.8733512420391778e-05,
      "loss": 1.6142,
      "step": 3850
    },
    {
      "epoch": 0.5591397849462365,
      "grad_norm": 96.2778091430664,
      "learning_rate": 1.8703329208294347e-05,
      "loss": 1.6471,
      "step": 3900
    },
    {
      "epoch": 0.5663082437275986,
      "grad_norm": 57.643821716308594,
      "learning_rate": 1.867314599619692e-05,
      "loss": 1.5491,
      "step": 3950
    },
    {
      "epoch": 0.5734767025089605,
      "grad_norm": 35.75929641723633,
      "learning_rate": 1.8642962784099484e-05,
      "loss": 1.6596,
      "step": 4000
    },
    {
      "epoch": 0.5806451612903226,
      "grad_norm": 36.89448928833008,
      "learning_rate": 1.8612779572002052e-05,
      "loss": 1.6438,
      "step": 4050
    },
    {
      "epoch": 0.5878136200716846,
      "grad_norm": 80.80117797851562,
      "learning_rate": 1.8582596359904624e-05,
      "loss": 1.7531,
      "step": 4100
    },
    {
      "epoch": 0.5949820788530465,
      "grad_norm": 78.25704193115234,
      "learning_rate": 1.8552413147807193e-05,
      "loss": 1.6402,
      "step": 4150
    },
    {
      "epoch": 0.6021505376344086,
      "grad_norm": 26.585716247558594,
      "learning_rate": 1.8522229935709758e-05,
      "loss": 1.5913,
      "step": 4200
    },
    {
      "epoch": 0.6093189964157706,
      "grad_norm": 86.67760467529297,
      "learning_rate": 1.849204672361233e-05,
      "loss": 1.6143,
      "step": 4250
    },
    {
      "epoch": 0.6164874551971327,
      "grad_norm": 88.61467742919922,
      "learning_rate": 1.8461863511514898e-05,
      "loss": 1.5779,
      "step": 4300
    },
    {
      "epoch": 0.6236559139784946,
      "grad_norm": 108.47418975830078,
      "learning_rate": 1.8432283963659413e-05,
      "loss": 1.6308,
      "step": 4350
    },
    {
      "epoch": 0.6308243727598566,
      "grad_norm": 66.7287368774414,
      "learning_rate": 1.840210075156198e-05,
      "loss": 1.7253,
      "step": 4400
    },
    {
      "epoch": 0.6379928315412187,
      "grad_norm": 87.00125122070312,
      "learning_rate": 1.8371917539464553e-05,
      "loss": 1.5943,
      "step": 4450
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 66.71989440917969,
      "learning_rate": 1.834173432736712e-05,
      "loss": 1.4956,
      "step": 4500
    },
    {
      "epoch": 0.6523297491039427,
      "grad_norm": 31.883338928222656,
      "learning_rate": 1.8311551115269687e-05,
      "loss": 1.5243,
      "step": 4550
    },
    {
      "epoch": 0.6594982078853047,
      "grad_norm": 51.31227111816406,
      "learning_rate": 1.828136790317226e-05,
      "loss": 1.5003,
      "step": 4600
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 38.90046691894531,
      "learning_rate": 1.8251184691074827e-05,
      "loss": 1.617,
      "step": 4650
    },
    {
      "epoch": 0.6738351254480287,
      "grad_norm": 103.26469421386719,
      "learning_rate": 1.8221001478977392e-05,
      "loss": 1.6499,
      "step": 4700
    },
    {
      "epoch": 0.6810035842293907,
      "grad_norm": 63.37947082519531,
      "learning_rate": 1.8190818266879964e-05,
      "loss": 1.584,
      "step": 4750
    },
    {
      "epoch": 0.6881720430107527,
      "grad_norm": 74.27564239501953,
      "learning_rate": 1.8160635054782533e-05,
      "loss": 1.5422,
      "step": 4800
    },
    {
      "epoch": 0.6953405017921147,
      "grad_norm": 99.98155212402344,
      "learning_rate": 1.8130451842685098e-05,
      "loss": 1.6406,
      "step": 4850
    },
    {
      "epoch": 0.7025089605734767,
      "grad_norm": 53.560604095458984,
      "learning_rate": 1.810026863058767e-05,
      "loss": 1.5312,
      "step": 4900
    },
    {
      "epoch": 0.7096774193548387,
      "grad_norm": 71.31781005859375,
      "learning_rate": 1.807008541849024e-05,
      "loss": 1.5775,
      "step": 4950
    },
    {
      "epoch": 0.7168458781362007,
      "grad_norm": 34.07607650756836,
      "learning_rate": 1.8039902206392807e-05,
      "loss": 1.6422,
      "step": 5000
    },
    {
      "epoch": 0.7240143369175627,
      "grad_norm": 51.42107391357422,
      "learning_rate": 1.8009718994295375e-05,
      "loss": 1.5511,
      "step": 5050
    },
    {
      "epoch": 0.7311827956989247,
      "grad_norm": 77.08232116699219,
      "learning_rate": 1.7979535782197944e-05,
      "loss": 1.6014,
      "step": 5100
    },
    {
      "epoch": 0.7383512544802867,
      "grad_norm": 102.47296142578125,
      "learning_rate": 1.7949352570100512e-05,
      "loss": 1.5089,
      "step": 5150
    },
    {
      "epoch": 0.7455197132616488,
      "grad_norm": 49.910221099853516,
      "learning_rate": 1.791916935800308e-05,
      "loss": 1.5552,
      "step": 5200
    },
    {
      "epoch": 0.7526881720430108,
      "grad_norm": 88.1629867553711,
      "learning_rate": 1.788898614590565e-05,
      "loss": 1.6657,
      "step": 5250
    },
    {
      "epoch": 0.7598566308243727,
      "grad_norm": 143.9617462158203,
      "learning_rate": 1.7858802933808218e-05,
      "loss": 1.6115,
      "step": 5300
    },
    {
      "epoch": 0.7670250896057348,
      "grad_norm": 88.4598388671875,
      "learning_rate": 1.7828619721710783e-05,
      "loss": 1.5889,
      "step": 5350
    },
    {
      "epoch": 0.7741935483870968,
      "grad_norm": 133.00912475585938,
      "learning_rate": 1.7798436509613355e-05,
      "loss": 1.539,
      "step": 5400
    },
    {
      "epoch": 0.7813620071684588,
      "grad_norm": 123.47762298583984,
      "learning_rate": 1.7768253297515924e-05,
      "loss": 1.464,
      "step": 5450
    },
    {
      "epoch": 0.7885304659498208,
      "grad_norm": 88.6274642944336,
      "learning_rate": 1.7738070085418492e-05,
      "loss": 1.5803,
      "step": 5500
    },
    {
      "epoch": 0.7956989247311828,
      "grad_norm": 85.1960220336914,
      "learning_rate": 1.770788687332106e-05,
      "loss": 1.6619,
      "step": 5550
    },
    {
      "epoch": 0.8028673835125448,
      "grad_norm": 67.58277893066406,
      "learning_rate": 1.767770366122363e-05,
      "loss": 1.4546,
      "step": 5600
    },
    {
      "epoch": 0.8100358422939068,
      "grad_norm": 96.83509063720703,
      "learning_rate": 1.7647520449126198e-05,
      "loss": 1.5103,
      "step": 5650
    },
    {
      "epoch": 0.8172043010752689,
      "grad_norm": 52.09833526611328,
      "learning_rate": 1.7617337237028766e-05,
      "loss": 1.6581,
      "step": 5700
    },
    {
      "epoch": 0.8243727598566308,
      "grad_norm": 68.51612091064453,
      "learning_rate": 1.7587154024931335e-05,
      "loss": 1.5091,
      "step": 5750
    },
    {
      "epoch": 0.8315412186379928,
      "grad_norm": 43.92729568481445,
      "learning_rate": 1.7556970812833903e-05,
      "loss": 1.5616,
      "step": 5800
    },
    {
      "epoch": 0.8387096774193549,
      "grad_norm": 50.5400505065918,
      "learning_rate": 1.7526787600736472e-05,
      "loss": 1.417,
      "step": 5850
    },
    {
      "epoch": 0.8458781362007168,
      "grad_norm": 41.58968734741211,
      "learning_rate": 1.749660438863904e-05,
      "loss": 1.6076,
      "step": 5900
    },
    {
      "epoch": 0.8530465949820788,
      "grad_norm": 134.29019165039062,
      "learning_rate": 1.746642117654161e-05,
      "loss": 1.5963,
      "step": 5950
    },
    {
      "epoch": 0.8602150537634409,
      "grad_norm": 85.39311218261719,
      "learning_rate": 1.7436237964444177e-05,
      "loss": 1.5891,
      "step": 6000
    },
    {
      "epoch": 0.8673835125448028,
      "grad_norm": 68.91819763183594,
      "learning_rate": 1.7406054752346746e-05,
      "loss": 1.5933,
      "step": 6050
    },
    {
      "epoch": 0.8745519713261649,
      "grad_norm": 136.87306213378906,
      "learning_rate": 1.7375871540249314e-05,
      "loss": 1.5421,
      "step": 6100
    },
    {
      "epoch": 0.8817204301075269,
      "grad_norm": 44.858192443847656,
      "learning_rate": 1.7345688328151883e-05,
      "loss": 1.4493,
      "step": 6150
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 76.70758819580078,
      "learning_rate": 1.731550511605445e-05,
      "loss": 1.5475,
      "step": 6200
    },
    {
      "epoch": 0.8960573476702509,
      "grad_norm": 150.49496459960938,
      "learning_rate": 1.728532190395702e-05,
      "loss": 1.5496,
      "step": 6250
    },
    {
      "epoch": 0.9032258064516129,
      "grad_norm": 116.65199279785156,
      "learning_rate": 1.725513869185959e-05,
      "loss": 1.4916,
      "step": 6300
    },
    {
      "epoch": 0.910394265232975,
      "grad_norm": 104.24698638916016,
      "learning_rate": 1.7224955479762157e-05,
      "loss": 1.571,
      "step": 6350
    },
    {
      "epoch": 0.9175627240143369,
      "grad_norm": 78.9054183959961,
      "learning_rate": 1.7195375931906675e-05,
      "loss": 1.5284,
      "step": 6400
    },
    {
      "epoch": 0.9247311827956989,
      "grad_norm": 106.78584289550781,
      "learning_rate": 1.7165192719809243e-05,
      "loss": 1.5845,
      "step": 6450
    },
    {
      "epoch": 0.931899641577061,
      "grad_norm": 60.75614547729492,
      "learning_rate": 1.7135009507711812e-05,
      "loss": 1.5817,
      "step": 6500
    },
    {
      "epoch": 0.9390681003584229,
      "grad_norm": 47.65946578979492,
      "learning_rate": 1.710482629561438e-05,
      "loss": 1.6222,
      "step": 6550
    },
    {
      "epoch": 0.946236559139785,
      "grad_norm": 48.58771896362305,
      "learning_rate": 1.707464308351695e-05,
      "loss": 1.5109,
      "step": 6600
    },
    {
      "epoch": 0.953405017921147,
      "grad_norm": 118.88809967041016,
      "learning_rate": 1.7044459871419517e-05,
      "loss": 1.4388,
      "step": 6650
    },
    {
      "epoch": 0.9605734767025089,
      "grad_norm": 39.9713249206543,
      "learning_rate": 1.7014276659322086e-05,
      "loss": 1.5587,
      "step": 6700
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 88.2680435180664,
      "learning_rate": 1.6984093447224655e-05,
      "loss": 1.6799,
      "step": 6750
    },
    {
      "epoch": 0.974910394265233,
      "grad_norm": 130.3756561279297,
      "learning_rate": 1.6953910235127223e-05,
      "loss": 1.5514,
      "step": 6800
    },
    {
      "epoch": 0.982078853046595,
      "grad_norm": 65.55506134033203,
      "learning_rate": 1.692372702302979e-05,
      "loss": 1.4668,
      "step": 6850
    },
    {
      "epoch": 0.989247311827957,
      "grad_norm": 59.22216033935547,
      "learning_rate": 1.689354381093236e-05,
      "loss": 1.5667,
      "step": 6900
    },
    {
      "epoch": 0.996415770609319,
      "grad_norm": 46.19839859008789,
      "learning_rate": 1.686336059883493e-05,
      "loss": 1.5624,
      "step": 6950
    },
    {
      "epoch": 1.003584229390681,
      "grad_norm": 103.88580322265625,
      "learning_rate": 1.6833177386737497e-05,
      "loss": 1.4835,
      "step": 7000
    },
    {
      "epoch": 1.010752688172043,
      "grad_norm": 60.89982223510742,
      "learning_rate": 1.6802994174640066e-05,
      "loss": 1.6211,
      "step": 7050
    },
    {
      "epoch": 1.017921146953405,
      "grad_norm": 26.76758575439453,
      "learning_rate": 1.6772810962542638e-05,
      "loss": 1.5226,
      "step": 7100
    },
    {
      "epoch": 1.025089605734767,
      "grad_norm": 38.08408737182617,
      "learning_rate": 1.6742627750445203e-05,
      "loss": 1.5114,
      "step": 7150
    },
    {
      "epoch": 1.032258064516129,
      "grad_norm": 82.14863586425781,
      "learning_rate": 1.671244453834777e-05,
      "loss": 1.5144,
      "step": 7200
    },
    {
      "epoch": 1.039426523297491,
      "grad_norm": 51.01852035522461,
      "learning_rate": 1.6682261326250343e-05,
      "loss": 1.5286,
      "step": 7250
    },
    {
      "epoch": 1.0465949820788532,
      "grad_norm": 70.28585815429688,
      "learning_rate": 1.6652078114152908e-05,
      "loss": 1.523,
      "step": 7300
    },
    {
      "epoch": 1.053763440860215,
      "grad_norm": 116.92529296875,
      "learning_rate": 1.6621894902055477e-05,
      "loss": 1.6291,
      "step": 7350
    },
    {
      "epoch": 1.060931899641577,
      "grad_norm": 154.8204803466797,
      "learning_rate": 1.659171168995805e-05,
      "loss": 1.4943,
      "step": 7400
    },
    {
      "epoch": 1.0681003584229392,
      "grad_norm": 37.76293182373047,
      "learning_rate": 1.6561528477860617e-05,
      "loss": 1.5578,
      "step": 7450
    },
    {
      "epoch": 1.075268817204301,
      "grad_norm": 68.72789764404297,
      "learning_rate": 1.6531345265763182e-05,
      "loss": 1.4962,
      "step": 7500
    },
    {
      "epoch": 1.082437275985663,
      "grad_norm": 37.386539459228516,
      "learning_rate": 1.6501162053665754e-05,
      "loss": 1.5473,
      "step": 7550
    },
    {
      "epoch": 1.0896057347670252,
      "grad_norm": 49.59426498413086,
      "learning_rate": 1.6470978841568323e-05,
      "loss": 1.4984,
      "step": 7600
    },
    {
      "epoch": 1.096774193548387,
      "grad_norm": 108.88502502441406,
      "learning_rate": 1.6440795629470888e-05,
      "loss": 1.5842,
      "step": 7650
    },
    {
      "epoch": 1.103942652329749,
      "grad_norm": 114.86710357666016,
      "learning_rate": 1.6410612417373457e-05,
      "loss": 1.5732,
      "step": 7700
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 44.28666687011719,
      "learning_rate": 1.638042920527603e-05,
      "loss": 1.589,
      "step": 7750
    },
    {
      "epoch": 1.118279569892473,
      "grad_norm": 52.58308792114258,
      "learning_rate": 1.6350245993178594e-05,
      "loss": 1.5328,
      "step": 7800
    },
    {
      "epoch": 1.125448028673835,
      "grad_norm": 99.17224884033203,
      "learning_rate": 1.6320062781081162e-05,
      "loss": 1.4704,
      "step": 7850
    },
    {
      "epoch": 1.1326164874551972,
      "grad_norm": 96.29908752441406,
      "learning_rate": 1.6289879568983734e-05,
      "loss": 1.5527,
      "step": 7900
    },
    {
      "epoch": 1.139784946236559,
      "grad_norm": 168.6223602294922,
      "learning_rate": 1.6259696356886303e-05,
      "loss": 1.5592,
      "step": 7950
    },
    {
      "epoch": 1.146953405017921,
      "grad_norm": 80.91637420654297,
      "learning_rate": 1.6229513144788868e-05,
      "loss": 1.5243,
      "step": 8000
    },
    {
      "epoch": 1.1541218637992832,
      "grad_norm": 158.60130310058594,
      "learning_rate": 1.619932993269144e-05,
      "loss": 1.4994,
      "step": 8050
    },
    {
      "epoch": 1.1612903225806452,
      "grad_norm": 28.900978088378906,
      "learning_rate": 1.6169146720594008e-05,
      "loss": 1.4736,
      "step": 8100
    },
    {
      "epoch": 1.168458781362007,
      "grad_norm": 149.56170654296875,
      "learning_rate": 1.6138963508496573e-05,
      "loss": 1.5209,
      "step": 8150
    },
    {
      "epoch": 1.1756272401433692,
      "grad_norm": 75.14207458496094,
      "learning_rate": 1.6108780296399145e-05,
      "loss": 1.5185,
      "step": 8200
    },
    {
      "epoch": 1.1827956989247312,
      "grad_norm": 219.6065216064453,
      "learning_rate": 1.6078597084301714e-05,
      "loss": 1.5099,
      "step": 8250
    },
    {
      "epoch": 1.189964157706093,
      "grad_norm": 47.33262252807617,
      "learning_rate": 1.604841387220428e-05,
      "loss": 1.5077,
      "step": 8300
    },
    {
      "epoch": 1.1971326164874552,
      "grad_norm": 135.7303009033203,
      "learning_rate": 1.601823066010685e-05,
      "loss": 1.489,
      "step": 8350
    },
    {
      "epoch": 1.2043010752688172,
      "grad_norm": 101.21601104736328,
      "learning_rate": 1.598804744800942e-05,
      "loss": 1.4408,
      "step": 8400
    },
    {
      "epoch": 1.2114695340501793,
      "grad_norm": 42.338436126708984,
      "learning_rate": 1.5957864235911988e-05,
      "loss": 1.4865,
      "step": 8450
    },
    {
      "epoch": 1.2186379928315412,
      "grad_norm": 47.71576690673828,
      "learning_rate": 1.5927681023814556e-05,
      "loss": 1.4143,
      "step": 8500
    },
    {
      "epoch": 1.2258064516129032,
      "grad_norm": 52.732154846191406,
      "learning_rate": 1.5898101475959074e-05,
      "loss": 1.5198,
      "step": 8550
    },
    {
      "epoch": 1.2329749103942653,
      "grad_norm": 95.99616241455078,
      "learning_rate": 1.5867918263861643e-05,
      "loss": 1.4248,
      "step": 8600
    },
    {
      "epoch": 1.2401433691756272,
      "grad_norm": 50.18438720703125,
      "learning_rate": 1.5837735051764208e-05,
      "loss": 1.552,
      "step": 8650
    },
    {
      "epoch": 1.2473118279569892,
      "grad_norm": 104.29383850097656,
      "learning_rate": 1.580755183966678e-05,
      "loss": 1.4393,
      "step": 8700
    },
    {
      "epoch": 1.2544802867383513,
      "grad_norm": 33.52751541137695,
      "learning_rate": 1.5777368627569348e-05,
      "loss": 1.5057,
      "step": 8750
    },
    {
      "epoch": 1.2616487455197132,
      "grad_norm": 50.45425033569336,
      "learning_rate": 1.5747185415471917e-05,
      "loss": 1.4935,
      "step": 8800
    },
    {
      "epoch": 1.2688172043010753,
      "grad_norm": 46.99325180053711,
      "learning_rate": 1.5717002203374485e-05,
      "loss": 1.5309,
      "step": 8850
    },
    {
      "epoch": 1.2759856630824373,
      "grad_norm": 59.82105255126953,
      "learning_rate": 1.5686818991277054e-05,
      "loss": 1.4769,
      "step": 8900
    },
    {
      "epoch": 1.2831541218637992,
      "grad_norm": 41.09029769897461,
      "learning_rate": 1.5656635779179622e-05,
      "loss": 1.4843,
      "step": 8950
    },
    {
      "epoch": 1.2903225806451613,
      "grad_norm": 34.844093322753906,
      "learning_rate": 1.562645256708219e-05,
      "loss": 1.5617,
      "step": 9000
    },
    {
      "epoch": 1.2974910394265233,
      "grad_norm": 84.11122131347656,
      "learning_rate": 1.559626935498476e-05,
      "loss": 1.4771,
      "step": 9050
    },
    {
      "epoch": 1.3046594982078852,
      "grad_norm": 37.84409713745117,
      "learning_rate": 1.5566086142887328e-05,
      "loss": 1.4894,
      "step": 9100
    },
    {
      "epoch": 1.3118279569892473,
      "grad_norm": 142.29200744628906,
      "learning_rate": 1.5535902930789896e-05,
      "loss": 1.4639,
      "step": 9150
    },
    {
      "epoch": 1.3189964157706093,
      "grad_norm": 94.52973937988281,
      "learning_rate": 1.5505719718692465e-05,
      "loss": 1.4623,
      "step": 9200
    },
    {
      "epoch": 1.3261648745519714,
      "grad_norm": 128.30914306640625,
      "learning_rate": 1.5475536506595033e-05,
      "loss": 1.6405,
      "step": 9250
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 117.87190246582031,
      "learning_rate": 1.5445353294497602e-05,
      "loss": 1.4261,
      "step": 9300
    },
    {
      "epoch": 1.3405017921146953,
      "grad_norm": 94.84635925292969,
      "learning_rate": 1.541517008240017e-05,
      "loss": 1.482,
      "step": 9350
    },
    {
      "epoch": 1.3476702508960574,
      "grad_norm": 120.51978302001953,
      "learning_rate": 1.538498687030274e-05,
      "loss": 1.601,
      "step": 9400
    },
    {
      "epoch": 1.3548387096774195,
      "grad_norm": 73.74388885498047,
      "learning_rate": 1.5354803658205308e-05,
      "loss": 1.5087,
      "step": 9450
    },
    {
      "epoch": 1.3620071684587813,
      "grad_norm": 56.90754699707031,
      "learning_rate": 1.5324620446107876e-05,
      "loss": 1.5018,
      "step": 9500
    },
    {
      "epoch": 1.3691756272401434,
      "grad_norm": 60.312347412109375,
      "learning_rate": 1.5294437234010445e-05,
      "loss": 1.541,
      "step": 9550
    },
    {
      "epoch": 1.3763440860215055,
      "grad_norm": 110.54373168945312,
      "learning_rate": 1.5264254021913013e-05,
      "loss": 1.3791,
      "step": 9600
    },
    {
      "epoch": 1.3835125448028673,
      "grad_norm": 52.47536087036133,
      "learning_rate": 1.5234070809815582e-05,
      "loss": 1.4797,
      "step": 9650
    },
    {
      "epoch": 1.3906810035842294,
      "grad_norm": 53.491851806640625,
      "learning_rate": 1.520388759771815e-05,
      "loss": 1.4345,
      "step": 9700
    },
    {
      "epoch": 1.3978494623655915,
      "grad_norm": 60.96763610839844,
      "learning_rate": 1.517370438562072e-05,
      "loss": 1.5399,
      "step": 9750
    },
    {
      "epoch": 1.4050179211469533,
      "grad_norm": 74.60835266113281,
      "learning_rate": 1.5143521173523287e-05,
      "loss": 1.5278,
      "step": 9800
    },
    {
      "epoch": 1.4121863799283154,
      "grad_norm": 41.164310455322266,
      "learning_rate": 1.5113337961425856e-05,
      "loss": 1.5167,
      "step": 9850
    },
    {
      "epoch": 1.4193548387096775,
      "grad_norm": 120.02440643310547,
      "learning_rate": 1.5083154749328426e-05,
      "loss": 1.415,
      "step": 9900
    },
    {
      "epoch": 1.4265232974910393,
      "grad_norm": 87.64510345458984,
      "learning_rate": 1.5052971537230994e-05,
      "loss": 1.6443,
      "step": 9950
    },
    {
      "epoch": 1.4336917562724014,
      "grad_norm": 78.70658111572266,
      "learning_rate": 1.5022788325133561e-05,
      "loss": 1.5165,
      "step": 10000
    },
    {
      "epoch": 1.4408602150537635,
      "grad_norm": 53.26738357543945,
      "learning_rate": 1.499260511303613e-05,
      "loss": 1.5823,
      "step": 10050
    },
    {
      "epoch": 1.4480286738351253,
      "grad_norm": 101.3155746459961,
      "learning_rate": 1.49624219009387e-05,
      "loss": 1.4634,
      "step": 10100
    },
    {
      "epoch": 1.4551971326164874,
      "grad_norm": 87.63397979736328,
      "learning_rate": 1.4932238688841267e-05,
      "loss": 1.4918,
      "step": 10150
    },
    {
      "epoch": 1.4623655913978495,
      "grad_norm": 82.1175765991211,
      "learning_rate": 1.4902055476743835e-05,
      "loss": 1.4462,
      "step": 10200
    },
    {
      "epoch": 1.4695340501792113,
      "grad_norm": 255.82289123535156,
      "learning_rate": 1.4871872264646406e-05,
      "loss": 1.5258,
      "step": 10250
    },
    {
      "epoch": 1.4767025089605734,
      "grad_norm": 44.35266876220703,
      "learning_rate": 1.4841689052548972e-05,
      "loss": 1.5028,
      "step": 10300
    },
    {
      "epoch": 1.4838709677419355,
      "grad_norm": 78.07976531982422,
      "learning_rate": 1.4811505840451541e-05,
      "loss": 1.4612,
      "step": 10350
    },
    {
      "epoch": 1.4910394265232976,
      "grad_norm": 55.724674224853516,
      "learning_rate": 1.4781322628354111e-05,
      "loss": 1.4529,
      "step": 10400
    },
    {
      "epoch": 1.4982078853046594,
      "grad_norm": 124.9554672241211,
      "learning_rate": 1.475113941625668e-05,
      "loss": 1.5303,
      "step": 10450
    },
    {
      "epoch": 1.5053763440860215,
      "grad_norm": 55.36139678955078,
      "learning_rate": 1.4720956204159247e-05,
      "loss": 1.4968,
      "step": 10500
    },
    {
      "epoch": 1.5125448028673834,
      "grad_norm": 179.84463500976562,
      "learning_rate": 1.4690772992061817e-05,
      "loss": 1.5371,
      "step": 10550
    },
    {
      "epoch": 1.5197132616487457,
      "grad_norm": 93.68927764892578,
      "learning_rate": 1.4661193444206335e-05,
      "loss": 1.5612,
      "step": 10600
    },
    {
      "epoch": 1.5268817204301075,
      "grad_norm": 68.56568908691406,
      "learning_rate": 1.4631010232108901e-05,
      "loss": 1.4827,
      "step": 10650
    },
    {
      "epoch": 1.5340501792114696,
      "grad_norm": 94.54997253417969,
      "learning_rate": 1.460082702001147e-05,
      "loss": 1.4552,
      "step": 10700
    },
    {
      "epoch": 1.5412186379928317,
      "grad_norm": 68.55359649658203,
      "learning_rate": 1.457064380791404e-05,
      "loss": 1.5071,
      "step": 10750
    },
    {
      "epoch": 1.5483870967741935,
      "grad_norm": 63.203678131103516,
      "learning_rate": 1.4540460595816609e-05,
      "loss": 1.4637,
      "step": 10800
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 39.28590774536133,
      "learning_rate": 1.4510277383719175e-05,
      "loss": 1.4839,
      "step": 10850
    },
    {
      "epoch": 1.5627240143369177,
      "grad_norm": 59.94184112548828,
      "learning_rate": 1.4480094171621746e-05,
      "loss": 1.5381,
      "step": 10900
    },
    {
      "epoch": 1.5698924731182795,
      "grad_norm": 54.84096908569336,
      "learning_rate": 1.4449910959524314e-05,
      "loss": 1.4981,
      "step": 10950
    },
    {
      "epoch": 1.5770609318996416,
      "grad_norm": 29.90532875061035,
      "learning_rate": 1.4419727747426881e-05,
      "loss": 1.4034,
      "step": 11000
    },
    {
      "epoch": 1.5842293906810037,
      "grad_norm": 63.23469543457031,
      "learning_rate": 1.4389544535329451e-05,
      "loss": 1.4769,
      "step": 11050
    },
    {
      "epoch": 1.5913978494623655,
      "grad_norm": 86.62483978271484,
      "learning_rate": 1.435936132323202e-05,
      "loss": 1.5484,
      "step": 11100
    },
    {
      "epoch": 1.5985663082437276,
      "grad_norm": 72.15178680419922,
      "learning_rate": 1.4329178111134587e-05,
      "loss": 1.4848,
      "step": 11150
    },
    {
      "epoch": 1.6057347670250897,
      "grad_norm": 140.89248657226562,
      "learning_rate": 1.4298994899037157e-05,
      "loss": 1.4043,
      "step": 11200
    },
    {
      "epoch": 1.6129032258064515,
      "grad_norm": 73.11458587646484,
      "learning_rate": 1.4268811686939725e-05,
      "loss": 1.3511,
      "step": 11250
    },
    {
      "epoch": 1.6200716845878136,
      "grad_norm": 81.74042510986328,
      "learning_rate": 1.4238628474842294e-05,
      "loss": 1.4537,
      "step": 11300
    },
    {
      "epoch": 1.6272401433691757,
      "grad_norm": 26.53163719177246,
      "learning_rate": 1.4208445262744862e-05,
      "loss": 1.4378,
      "step": 11350
    },
    {
      "epoch": 1.6344086021505375,
      "grad_norm": 56.086273193359375,
      "learning_rate": 1.4178262050647431e-05,
      "loss": 1.4681,
      "step": 11400
    },
    {
      "epoch": 1.6415770609318996,
      "grad_norm": 44.384334564208984,
      "learning_rate": 1.414807883855e-05,
      "loss": 1.4574,
      "step": 11450
    },
    {
      "epoch": 1.6487455197132617,
      "grad_norm": 23.343400955200195,
      "learning_rate": 1.411789562645257e-05,
      "loss": 1.4568,
      "step": 11500
    },
    {
      "epoch": 1.6559139784946235,
      "grad_norm": 95.39313507080078,
      "learning_rate": 1.4087712414355137e-05,
      "loss": 1.5504,
      "step": 11550
    },
    {
      "epoch": 1.6630824372759858,
      "grad_norm": 140.87954711914062,
      "learning_rate": 1.4057529202257705e-05,
      "loss": 1.5491,
      "step": 11600
    },
    {
      "epoch": 1.6702508960573477,
      "grad_norm": 167.1306610107422,
      "learning_rate": 1.4027345990160275e-05,
      "loss": 1.4045,
      "step": 11650
    },
    {
      "epoch": 1.6774193548387095,
      "grad_norm": 213.2017059326172,
      "learning_rate": 1.3997162778062842e-05,
      "loss": 1.4012,
      "step": 11700
    },
    {
      "epoch": 1.6845878136200718,
      "grad_norm": 55.07118225097656,
      "learning_rate": 1.396697956596541e-05,
      "loss": 1.4351,
      "step": 11750
    },
    {
      "epoch": 1.6917562724014337,
      "grad_norm": 52.074222564697266,
      "learning_rate": 1.393679635386798e-05,
      "loss": 1.4464,
      "step": 11800
    },
    {
      "epoch": 1.6989247311827957,
      "grad_norm": 122.38277435302734,
      "learning_rate": 1.390661314177055e-05,
      "loss": 1.4854,
      "step": 11850
    },
    {
      "epoch": 1.7060931899641578,
      "grad_norm": 46.43083953857422,
      "learning_rate": 1.3876429929673116e-05,
      "loss": 1.4464,
      "step": 11900
    },
    {
      "epoch": 1.7132616487455197,
      "grad_norm": 69.0225830078125,
      "learning_rate": 1.3846246717575686e-05,
      "loss": 1.4205,
      "step": 11950
    },
    {
      "epoch": 1.7204301075268817,
      "grad_norm": 39.09453201293945,
      "learning_rate": 1.3816063505478255e-05,
      "loss": 1.4483,
      "step": 12000
    },
    {
      "epoch": 1.7275985663082438,
      "grad_norm": 41.04753112792969,
      "learning_rate": 1.3785880293380822e-05,
      "loss": 1.4164,
      "step": 12050
    },
    {
      "epoch": 1.7347670250896057,
      "grad_norm": 41.55251693725586,
      "learning_rate": 1.3755697081283392e-05,
      "loss": 1.496,
      "step": 12100
    },
    {
      "epoch": 1.7419354838709677,
      "grad_norm": 59.118675231933594,
      "learning_rate": 1.372551386918596e-05,
      "loss": 1.4254,
      "step": 12150
    },
    {
      "epoch": 1.7491039426523298,
      "grad_norm": 56.720027923583984,
      "learning_rate": 1.3695330657088527e-05,
      "loss": 1.4303,
      "step": 12200
    },
    {
      "epoch": 1.7562724014336917,
      "grad_norm": 83.83139038085938,
      "learning_rate": 1.3665147444991098e-05,
      "loss": 1.5528,
      "step": 12250
    },
    {
      "epoch": 1.7634408602150538,
      "grad_norm": 70.981201171875,
      "learning_rate": 1.3634964232893666e-05,
      "loss": 1.4074,
      "step": 12300
    },
    {
      "epoch": 1.7706093189964158,
      "grad_norm": 60.70819091796875,
      "learning_rate": 1.3604781020796235e-05,
      "loss": 1.5184,
      "step": 12350
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 49.339149475097656,
      "learning_rate": 1.3574597808698803e-05,
      "loss": 1.5472,
      "step": 12400
    },
    {
      "epoch": 1.7849462365591398,
      "grad_norm": 55.4351806640625,
      "learning_rate": 1.3544414596601372e-05,
      "loss": 1.5292,
      "step": 12450
    },
    {
      "epoch": 1.7921146953405018,
      "grad_norm": 81.33860778808594,
      "learning_rate": 1.351423138450394e-05,
      "loss": 1.4718,
      "step": 12500
    },
    {
      "epoch": 1.7992831541218637,
      "grad_norm": 37.59245300292969,
      "learning_rate": 1.3484048172406507e-05,
      "loss": 1.4049,
      "step": 12550
    },
    {
      "epoch": 1.8064516129032258,
      "grad_norm": 140.53659057617188,
      "learning_rate": 1.3453864960309077e-05,
      "loss": 1.4645,
      "step": 12600
    },
    {
      "epoch": 1.8136200716845878,
      "grad_norm": 81.18183898925781,
      "learning_rate": 1.3424285412453595e-05,
      "loss": 1.4903,
      "step": 12650
    },
    {
      "epoch": 1.8207885304659497,
      "grad_norm": 213.79981994628906,
      "learning_rate": 1.3394102200356164e-05,
      "loss": 1.458,
      "step": 12700
    },
    {
      "epoch": 1.827956989247312,
      "grad_norm": 94.62178802490234,
      "learning_rate": 1.336391898825873e-05,
      "loss": 1.5073,
      "step": 12750
    },
    {
      "epoch": 1.8351254480286738,
      "grad_norm": 81.14270782470703,
      "learning_rate": 1.33337357761613e-05,
      "loss": 1.5471,
      "step": 12800
    },
    {
      "epoch": 1.8422939068100357,
      "grad_norm": 64.89045715332031,
      "learning_rate": 1.3303552564063869e-05,
      "loss": 1.4753,
      "step": 12850
    },
    {
      "epoch": 1.849462365591398,
      "grad_norm": 51.52842330932617,
      "learning_rate": 1.3273369351966436e-05,
      "loss": 1.4578,
      "step": 12900
    },
    {
      "epoch": 1.8566308243727598,
      "grad_norm": 56.6365852355957,
      "learning_rate": 1.3243186139869006e-05,
      "loss": 1.4555,
      "step": 12950
    },
    {
      "epoch": 1.863799283154122,
      "grad_norm": 33.43904113769531,
      "learning_rate": 1.3213002927771575e-05,
      "loss": 1.4767,
      "step": 13000
    },
    {
      "epoch": 1.870967741935484,
      "grad_norm": 74.93965148925781,
      "learning_rate": 1.3182819715674142e-05,
      "loss": 1.4203,
      "step": 13050
    },
    {
      "epoch": 1.8781362007168458,
      "grad_norm": 67.09234619140625,
      "learning_rate": 1.3152636503576712e-05,
      "loss": 1.4096,
      "step": 13100
    },
    {
      "epoch": 1.885304659498208,
      "grad_norm": 100.77851104736328,
      "learning_rate": 1.312245329147928e-05,
      "loss": 1.4275,
      "step": 13150
    },
    {
      "epoch": 1.89247311827957,
      "grad_norm": 33.40199661254883,
      "learning_rate": 1.3092270079381849e-05,
      "loss": 1.4961,
      "step": 13200
    },
    {
      "epoch": 1.8996415770609318,
      "grad_norm": 92.5815200805664,
      "learning_rate": 1.3062086867284417e-05,
      "loss": 1.5342,
      "step": 13250
    },
    {
      "epoch": 1.906810035842294,
      "grad_norm": 46.04892349243164,
      "learning_rate": 1.3031903655186986e-05,
      "loss": 1.4686,
      "step": 13300
    },
    {
      "epoch": 1.913978494623656,
      "grad_norm": 138.27313232421875,
      "learning_rate": 1.3001720443089554e-05,
      "loss": 1.4384,
      "step": 13350
    },
    {
      "epoch": 1.9211469534050178,
      "grad_norm": 74.4681167602539,
      "learning_rate": 1.2971537230992125e-05,
      "loss": 1.529,
      "step": 13400
    },
    {
      "epoch": 1.92831541218638,
      "grad_norm": 91.0648193359375,
      "learning_rate": 1.2941354018894691e-05,
      "loss": 1.4044,
      "step": 13450
    },
    {
      "epoch": 1.935483870967742,
      "grad_norm": 43.870811462402344,
      "learning_rate": 1.291117080679726e-05,
      "loss": 1.4619,
      "step": 13500
    },
    {
      "epoch": 1.9426523297491038,
      "grad_norm": 66.37284851074219,
      "learning_rate": 1.288098759469983e-05,
      "loss": 1.4771,
      "step": 13550
    },
    {
      "epoch": 1.949820788530466,
      "grad_norm": 59.23664855957031,
      "learning_rate": 1.2850804382602397e-05,
      "loss": 1.5329,
      "step": 13600
    },
    {
      "epoch": 1.956989247311828,
      "grad_norm": 159.31455993652344,
      "learning_rate": 1.2820621170504966e-05,
      "loss": 1.4054,
      "step": 13650
    },
    {
      "epoch": 1.9641577060931898,
      "grad_norm": 73.12272644042969,
      "learning_rate": 1.2790437958407536e-05,
      "loss": 1.4898,
      "step": 13700
    },
    {
      "epoch": 1.971326164874552,
      "grad_norm": 48.770320892333984,
      "learning_rate": 1.2760254746310104e-05,
      "loss": 1.4906,
      "step": 13750
    },
    {
      "epoch": 1.978494623655914,
      "grad_norm": 29.840179443359375,
      "learning_rate": 1.2730071534212671e-05,
      "loss": 1.5635,
      "step": 13800
    },
    {
      "epoch": 1.9856630824372759,
      "grad_norm": 57.19105911254883,
      "learning_rate": 1.2699888322115241e-05,
      "loss": 1.4983,
      "step": 13850
    },
    {
      "epoch": 1.9928315412186381,
      "grad_norm": 58.423683166503906,
      "learning_rate": 1.266970511001781e-05,
      "loss": 1.4589,
      "step": 13900
    },
    {
      "epoch": 2.0,
      "grad_norm": 200.73870849609375,
      "learning_rate": 1.2639521897920377e-05,
      "loss": 1.444,
      "step": 13950
    }
  ],
  "logging_steps": 50,
  "max_steps": 34875,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.7041886902977823e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
