{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 20925,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007168458781362007,
      "grad_norm": 94.6792221069336,
      "learning_rate": 4.931192660550459e-07,
      "loss": 3.5811,
      "step": 50
    },
    {
      "epoch": 0.014336917562724014,
      "grad_norm": 111.95091247558594,
      "learning_rate": 1.06651376146789e-06,
      "loss": 3.5112,
      "step": 100
    },
    {
      "epoch": 0.021505376344086023,
      "grad_norm": 63.02651596069336,
      "learning_rate": 1.639908256880734e-06,
      "loss": 3.3545,
      "step": 150
    },
    {
      "epoch": 0.02867383512544803,
      "grad_norm": 65.95425415039062,
      "learning_rate": 2.213302752293578e-06,
      "loss": 3.0684,
      "step": 200
    },
    {
      "epoch": 0.035842293906810034,
      "grad_norm": 25.89705467224121,
      "learning_rate": 2.7866972477064223e-06,
      "loss": 2.8748,
      "step": 250
    },
    {
      "epoch": 0.043010752688172046,
      "grad_norm": 88.13428497314453,
      "learning_rate": 3.3600917431192665e-06,
      "loss": 2.5508,
      "step": 300
    },
    {
      "epoch": 0.05017921146953405,
      "grad_norm": 45.88043975830078,
      "learning_rate": 3.93348623853211e-06,
      "loss": 2.3945,
      "step": 350
    },
    {
      "epoch": 0.05734767025089606,
      "grad_norm": 75.59368133544922,
      "learning_rate": 4.5068807339449545e-06,
      "loss": 2.2547,
      "step": 400
    },
    {
      "epoch": 0.06451612903225806,
      "grad_norm": 35.772552490234375,
      "learning_rate": 5.080275229357799e-06,
      "loss": 2.138,
      "step": 450
    },
    {
      "epoch": 0.07168458781362007,
      "grad_norm": 43.26051330566406,
      "learning_rate": 5.653669724770643e-06,
      "loss": 1.9956,
      "step": 500
    },
    {
      "epoch": 0.07885304659498207,
      "grad_norm": 94.88484191894531,
      "learning_rate": 6.227064220183486e-06,
      "loss": 2.0355,
      "step": 550
    },
    {
      "epoch": 0.08602150537634409,
      "grad_norm": 91.1935043334961,
      "learning_rate": 6.8004587155963305e-06,
      "loss": 1.9752,
      "step": 600
    },
    {
      "epoch": 0.0931899641577061,
      "grad_norm": 55.290164947509766,
      "learning_rate": 7.3738532110091755e-06,
      "loss": 1.9225,
      "step": 650
    },
    {
      "epoch": 0.1003584229390681,
      "grad_norm": 40.3649787902832,
      "learning_rate": 7.947247706422018e-06,
      "loss": 1.8897,
      "step": 700
    },
    {
      "epoch": 0.10752688172043011,
      "grad_norm": 48.57997131347656,
      "learning_rate": 8.520642201834864e-06,
      "loss": 1.9114,
      "step": 750
    },
    {
      "epoch": 0.11469534050179211,
      "grad_norm": 44.484527587890625,
      "learning_rate": 9.094036697247706e-06,
      "loss": 1.8746,
      "step": 800
    },
    {
      "epoch": 0.12186379928315412,
      "grad_norm": 72.00469970703125,
      "learning_rate": 9.66743119266055e-06,
      "loss": 1.9188,
      "step": 850
    },
    {
      "epoch": 0.12903225806451613,
      "grad_norm": 85.95138549804688,
      "learning_rate": 1.0240825688073395e-05,
      "loss": 1.8603,
      "step": 900
    },
    {
      "epoch": 0.13620071684587814,
      "grad_norm": 34.68693923950195,
      "learning_rate": 1.0814220183486239e-05,
      "loss": 1.8837,
      "step": 950
    },
    {
      "epoch": 0.14336917562724014,
      "grad_norm": 116.72151184082031,
      "learning_rate": 1.1387614678899083e-05,
      "loss": 1.8065,
      "step": 1000
    },
    {
      "epoch": 0.15053763440860216,
      "grad_norm": 100.70439147949219,
      "learning_rate": 1.1961009174311929e-05,
      "loss": 1.8459,
      "step": 1050
    },
    {
      "epoch": 0.15770609318996415,
      "grad_norm": 69.32882690429688,
      "learning_rate": 1.253440366972477e-05,
      "loss": 1.712,
      "step": 1100
    },
    {
      "epoch": 0.16487455197132617,
      "grad_norm": 131.72879028320312,
      "learning_rate": 1.3107798165137616e-05,
      "loss": 1.7172,
      "step": 1150
    },
    {
      "epoch": 0.17204301075268819,
      "grad_norm": 80.95945739746094,
      "learning_rate": 1.368119266055046e-05,
      "loss": 1.7857,
      "step": 1200
    },
    {
      "epoch": 0.17921146953405018,
      "grad_norm": 85.78988647460938,
      "learning_rate": 1.4254587155963304e-05,
      "loss": 1.781,
      "step": 1250
    },
    {
      "epoch": 0.1863799283154122,
      "grad_norm": 62.13663101196289,
      "learning_rate": 1.4827981651376148e-05,
      "loss": 1.7072,
      "step": 1300
    },
    {
      "epoch": 0.1935483870967742,
      "grad_norm": 60.527957916259766,
      "learning_rate": 1.5401376146788993e-05,
      "loss": 1.8125,
      "step": 1350
    },
    {
      "epoch": 0.2007168458781362,
      "grad_norm": 39.60439682006836,
      "learning_rate": 1.5974770642201837e-05,
      "loss": 1.6855,
      "step": 1400
    },
    {
      "epoch": 0.2078853046594982,
      "grad_norm": 119.29389190673828,
      "learning_rate": 1.654816513761468e-05,
      "loss": 1.7185,
      "step": 1450
    },
    {
      "epoch": 0.21505376344086022,
      "grad_norm": 48.4014778137207,
      "learning_rate": 1.7121559633027525e-05,
      "loss": 1.8392,
      "step": 1500
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 74.41249084472656,
      "learning_rate": 1.769495412844037e-05,
      "loss": 1.7089,
      "step": 1550
    },
    {
      "epoch": 0.22939068100358423,
      "grad_norm": 80.64263916015625,
      "learning_rate": 1.826834862385321e-05,
      "loss": 1.7053,
      "step": 1600
    },
    {
      "epoch": 0.23655913978494625,
      "grad_norm": 75.54520416259766,
      "learning_rate": 1.8841743119266055e-05,
      "loss": 1.7721,
      "step": 1650
    },
    {
      "epoch": 0.24372759856630824,
      "grad_norm": 95.68910217285156,
      "learning_rate": 1.9415137614678902e-05,
      "loss": 1.8003,
      "step": 1700
    },
    {
      "epoch": 0.25089605734767023,
      "grad_norm": 88.0743637084961,
      "learning_rate": 1.9988532110091746e-05,
      "loss": 1.7913,
      "step": 1750
    },
    {
      "epoch": 0.25806451612903225,
      "grad_norm": 84.54595947265625,
      "learning_rate": 1.997042045214452e-05,
      "loss": 1.7547,
      "step": 1800
    },
    {
      "epoch": 0.26523297491039427,
      "grad_norm": 62.25143051147461,
      "learning_rate": 1.9940237240047088e-05,
      "loss": 1.7842,
      "step": 1850
    },
    {
      "epoch": 0.2724014336917563,
      "grad_norm": 132.93038940429688,
      "learning_rate": 1.9910054027949656e-05,
      "loss": 1.7194,
      "step": 1900
    },
    {
      "epoch": 0.27956989247311825,
      "grad_norm": 114.3082046508789,
      "learning_rate": 1.9879870815852225e-05,
      "loss": 1.7241,
      "step": 1950
    },
    {
      "epoch": 0.2867383512544803,
      "grad_norm": 78.41365051269531,
      "learning_rate": 1.9849687603754794e-05,
      "loss": 1.8103,
      "step": 2000
    },
    {
      "epoch": 0.2939068100358423,
      "grad_norm": 111.15614318847656,
      "learning_rate": 1.9819504391657362e-05,
      "loss": 1.722,
      "step": 2050
    },
    {
      "epoch": 0.3010752688172043,
      "grad_norm": 64.8713150024414,
      "learning_rate": 1.978932117955993e-05,
      "loss": 1.7172,
      "step": 2100
    },
    {
      "epoch": 0.30824372759856633,
      "grad_norm": 52.56601333618164,
      "learning_rate": 1.97591379674625e-05,
      "loss": 1.748,
      "step": 2150
    },
    {
      "epoch": 0.3154121863799283,
      "grad_norm": 48.4078369140625,
      "learning_rate": 1.9729558419607017e-05,
      "loss": 1.6385,
      "step": 2200
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 49.44916915893555,
      "learning_rate": 1.9699375207509585e-05,
      "loss": 1.7564,
      "step": 2250
    },
    {
      "epoch": 0.32974910394265233,
      "grad_norm": 78.5113525390625,
      "learning_rate": 1.9669191995412154e-05,
      "loss": 1.8061,
      "step": 2300
    },
    {
      "epoch": 0.33691756272401435,
      "grad_norm": 89.51522827148438,
      "learning_rate": 1.9639008783314722e-05,
      "loss": 1.7959,
      "step": 2350
    },
    {
      "epoch": 0.34408602150537637,
      "grad_norm": 77.31348419189453,
      "learning_rate": 1.960882557121729e-05,
      "loss": 1.7281,
      "step": 2400
    },
    {
      "epoch": 0.35125448028673834,
      "grad_norm": 35.14601135253906,
      "learning_rate": 1.957864235911986e-05,
      "loss": 1.7025,
      "step": 2450
    },
    {
      "epoch": 0.35842293906810035,
      "grad_norm": 170.42095947265625,
      "learning_rate": 1.9548459147022428e-05,
      "loss": 1.6293,
      "step": 2500
    },
    {
      "epoch": 0.3655913978494624,
      "grad_norm": 115.96540069580078,
      "learning_rate": 1.9518275934924997e-05,
      "loss": 1.6109,
      "step": 2550
    },
    {
      "epoch": 0.3727598566308244,
      "grad_norm": 78.90654754638672,
      "learning_rate": 1.9488092722827565e-05,
      "loss": 1.6356,
      "step": 2600
    },
    {
      "epoch": 0.37992831541218636,
      "grad_norm": 91.19440460205078,
      "learning_rate": 1.9457909510730134e-05,
      "loss": 1.665,
      "step": 2650
    },
    {
      "epoch": 0.3870967741935484,
      "grad_norm": 67.31293487548828,
      "learning_rate": 1.9427726298632702e-05,
      "loss": 1.6013,
      "step": 2700
    },
    {
      "epoch": 0.3942652329749104,
      "grad_norm": 35.623600006103516,
      "learning_rate": 1.939754308653527e-05,
      "loss": 1.7006,
      "step": 2750
    },
    {
      "epoch": 0.4014336917562724,
      "grad_norm": 57.7923469543457,
      "learning_rate": 1.936735987443784e-05,
      "loss": 1.595,
      "step": 2800
    },
    {
      "epoch": 0.40860215053763443,
      "grad_norm": 53.78890609741211,
      "learning_rate": 1.9337176662340408e-05,
      "loss": 1.6378,
      "step": 2850
    },
    {
      "epoch": 0.4157706093189964,
      "grad_norm": 31.879650115966797,
      "learning_rate": 1.9306993450242976e-05,
      "loss": 1.6242,
      "step": 2900
    },
    {
      "epoch": 0.4229390681003584,
      "grad_norm": 41.607337951660156,
      "learning_rate": 1.9276810238145545e-05,
      "loss": 1.7083,
      "step": 2950
    },
    {
      "epoch": 0.43010752688172044,
      "grad_norm": 75.56098937988281,
      "learning_rate": 1.9246627026048113e-05,
      "loss": 1.6862,
      "step": 3000
    },
    {
      "epoch": 0.43727598566308246,
      "grad_norm": 66.35958099365234,
      "learning_rate": 1.9216443813950682e-05,
      "loss": 1.638,
      "step": 3050
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 52.193912506103516,
      "learning_rate": 1.918626060185325e-05,
      "loss": 1.6719,
      "step": 3100
    },
    {
      "epoch": 0.45161290322580644,
      "grad_norm": 66.34355163574219,
      "learning_rate": 1.915607738975582e-05,
      "loss": 1.6849,
      "step": 3150
    },
    {
      "epoch": 0.45878136200716846,
      "grad_norm": 52.57299041748047,
      "learning_rate": 1.9125894177658387e-05,
      "loss": 1.7525,
      "step": 3200
    },
    {
      "epoch": 0.4659498207885305,
      "grad_norm": 59.958160400390625,
      "learning_rate": 1.9095710965560956e-05,
      "loss": 1.7571,
      "step": 3250
    },
    {
      "epoch": 0.4731182795698925,
      "grad_norm": 53.841007232666016,
      "learning_rate": 1.9065527753463524e-05,
      "loss": 1.5363,
      "step": 3300
    },
    {
      "epoch": 0.48028673835125446,
      "grad_norm": 49.62135696411133,
      "learning_rate": 1.9035344541366093e-05,
      "loss": 1.5725,
      "step": 3350
    },
    {
      "epoch": 0.4874551971326165,
      "grad_norm": 31.244384765625,
      "learning_rate": 1.900516132926866e-05,
      "loss": 1.7516,
      "step": 3400
    },
    {
      "epoch": 0.4946236559139785,
      "grad_norm": 77.0704574584961,
      "learning_rate": 1.897497811717123e-05,
      "loss": 1.4213,
      "step": 3450
    },
    {
      "epoch": 0.5017921146953405,
      "grad_norm": 80.37962341308594,
      "learning_rate": 1.89447949050738e-05,
      "loss": 1.713,
      "step": 3500
    },
    {
      "epoch": 0.5089605734767025,
      "grad_norm": 164.86434936523438,
      "learning_rate": 1.8914611692976367e-05,
      "loss": 1.6249,
      "step": 3550
    },
    {
      "epoch": 0.5161290322580645,
      "grad_norm": 57.523372650146484,
      "learning_rate": 1.8884428480878936e-05,
      "loss": 1.6016,
      "step": 3600
    },
    {
      "epoch": 0.5232974910394266,
      "grad_norm": 50.754878997802734,
      "learning_rate": 1.8854245268781507e-05,
      "loss": 1.6603,
      "step": 3650
    },
    {
      "epoch": 0.5304659498207885,
      "grad_norm": 55.45808029174805,
      "learning_rate": 1.8824062056684073e-05,
      "loss": 1.7404,
      "step": 3700
    },
    {
      "epoch": 0.5376344086021505,
      "grad_norm": 37.93119430541992,
      "learning_rate": 1.879387884458664e-05,
      "loss": 1.6315,
      "step": 3750
    },
    {
      "epoch": 0.5448028673835126,
      "grad_norm": 135.80096435546875,
      "learning_rate": 1.8763695632489213e-05,
      "loss": 1.5807,
      "step": 3800
    },
    {
      "epoch": 0.5519713261648745,
      "grad_norm": 59.26798629760742,
      "learning_rate": 1.8733512420391778e-05,
      "loss": 1.6142,
      "step": 3850
    },
    {
      "epoch": 0.5591397849462365,
      "grad_norm": 96.2778091430664,
      "learning_rate": 1.8703329208294347e-05,
      "loss": 1.6471,
      "step": 3900
    },
    {
      "epoch": 0.5663082437275986,
      "grad_norm": 57.643821716308594,
      "learning_rate": 1.867314599619692e-05,
      "loss": 1.5491,
      "step": 3950
    },
    {
      "epoch": 0.5734767025089605,
      "grad_norm": 35.75929641723633,
      "learning_rate": 1.8642962784099484e-05,
      "loss": 1.6596,
      "step": 4000
    },
    {
      "epoch": 0.5806451612903226,
      "grad_norm": 36.89448928833008,
      "learning_rate": 1.8612779572002052e-05,
      "loss": 1.6438,
      "step": 4050
    },
    {
      "epoch": 0.5878136200716846,
      "grad_norm": 80.80117797851562,
      "learning_rate": 1.8582596359904624e-05,
      "loss": 1.7531,
      "step": 4100
    },
    {
      "epoch": 0.5949820788530465,
      "grad_norm": 78.25704193115234,
      "learning_rate": 1.8552413147807193e-05,
      "loss": 1.6402,
      "step": 4150
    },
    {
      "epoch": 0.6021505376344086,
      "grad_norm": 26.585716247558594,
      "learning_rate": 1.8522229935709758e-05,
      "loss": 1.5913,
      "step": 4200
    },
    {
      "epoch": 0.6093189964157706,
      "grad_norm": 86.67760467529297,
      "learning_rate": 1.849204672361233e-05,
      "loss": 1.6143,
      "step": 4250
    },
    {
      "epoch": 0.6164874551971327,
      "grad_norm": 88.61467742919922,
      "learning_rate": 1.8461863511514898e-05,
      "loss": 1.5779,
      "step": 4300
    },
    {
      "epoch": 0.6236559139784946,
      "grad_norm": 108.47418975830078,
      "learning_rate": 1.8432283963659413e-05,
      "loss": 1.6308,
      "step": 4350
    },
    {
      "epoch": 0.6308243727598566,
      "grad_norm": 66.7287368774414,
      "learning_rate": 1.840210075156198e-05,
      "loss": 1.7253,
      "step": 4400
    },
    {
      "epoch": 0.6379928315412187,
      "grad_norm": 87.00125122070312,
      "learning_rate": 1.8371917539464553e-05,
      "loss": 1.5943,
      "step": 4450
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 66.71989440917969,
      "learning_rate": 1.834173432736712e-05,
      "loss": 1.4956,
      "step": 4500
    },
    {
      "epoch": 0.6523297491039427,
      "grad_norm": 31.883338928222656,
      "learning_rate": 1.8311551115269687e-05,
      "loss": 1.5243,
      "step": 4550
    },
    {
      "epoch": 0.6594982078853047,
      "grad_norm": 51.31227111816406,
      "learning_rate": 1.828136790317226e-05,
      "loss": 1.5003,
      "step": 4600
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 38.90046691894531,
      "learning_rate": 1.8251184691074827e-05,
      "loss": 1.617,
      "step": 4650
    },
    {
      "epoch": 0.6738351254480287,
      "grad_norm": 103.26469421386719,
      "learning_rate": 1.8221001478977392e-05,
      "loss": 1.6499,
      "step": 4700
    },
    {
      "epoch": 0.6810035842293907,
      "grad_norm": 63.37947082519531,
      "learning_rate": 1.8190818266879964e-05,
      "loss": 1.584,
      "step": 4750
    },
    {
      "epoch": 0.6881720430107527,
      "grad_norm": 74.27564239501953,
      "learning_rate": 1.8160635054782533e-05,
      "loss": 1.5422,
      "step": 4800
    },
    {
      "epoch": 0.6953405017921147,
      "grad_norm": 99.98155212402344,
      "learning_rate": 1.8130451842685098e-05,
      "loss": 1.6406,
      "step": 4850
    },
    {
      "epoch": 0.7025089605734767,
      "grad_norm": 53.560604095458984,
      "learning_rate": 1.810026863058767e-05,
      "loss": 1.5312,
      "step": 4900
    },
    {
      "epoch": 0.7096774193548387,
      "grad_norm": 71.31781005859375,
      "learning_rate": 1.807008541849024e-05,
      "loss": 1.5775,
      "step": 4950
    },
    {
      "epoch": 0.7168458781362007,
      "grad_norm": 34.07607650756836,
      "learning_rate": 1.8039902206392807e-05,
      "loss": 1.6422,
      "step": 5000
    },
    {
      "epoch": 0.7240143369175627,
      "grad_norm": 51.42107391357422,
      "learning_rate": 1.8009718994295375e-05,
      "loss": 1.5511,
      "step": 5050
    },
    {
      "epoch": 0.7311827956989247,
      "grad_norm": 77.08232116699219,
      "learning_rate": 1.7979535782197944e-05,
      "loss": 1.6014,
      "step": 5100
    },
    {
      "epoch": 0.7383512544802867,
      "grad_norm": 102.47296142578125,
      "learning_rate": 1.7949352570100512e-05,
      "loss": 1.5089,
      "step": 5150
    },
    {
      "epoch": 0.7455197132616488,
      "grad_norm": 49.910221099853516,
      "learning_rate": 1.791916935800308e-05,
      "loss": 1.5552,
      "step": 5200
    },
    {
      "epoch": 0.7526881720430108,
      "grad_norm": 88.1629867553711,
      "learning_rate": 1.788898614590565e-05,
      "loss": 1.6657,
      "step": 5250
    },
    {
      "epoch": 0.7598566308243727,
      "grad_norm": 143.9617462158203,
      "learning_rate": 1.7858802933808218e-05,
      "loss": 1.6115,
      "step": 5300
    },
    {
      "epoch": 0.7670250896057348,
      "grad_norm": 88.4598388671875,
      "learning_rate": 1.7828619721710783e-05,
      "loss": 1.5889,
      "step": 5350
    },
    {
      "epoch": 0.7741935483870968,
      "grad_norm": 133.00912475585938,
      "learning_rate": 1.7798436509613355e-05,
      "loss": 1.539,
      "step": 5400
    },
    {
      "epoch": 0.7813620071684588,
      "grad_norm": 123.47762298583984,
      "learning_rate": 1.7768253297515924e-05,
      "loss": 1.464,
      "step": 5450
    },
    {
      "epoch": 0.7885304659498208,
      "grad_norm": 88.6274642944336,
      "learning_rate": 1.7738070085418492e-05,
      "loss": 1.5803,
      "step": 5500
    },
    {
      "epoch": 0.7956989247311828,
      "grad_norm": 85.1960220336914,
      "learning_rate": 1.770788687332106e-05,
      "loss": 1.6619,
      "step": 5550
    },
    {
      "epoch": 0.8028673835125448,
      "grad_norm": 67.58277893066406,
      "learning_rate": 1.767770366122363e-05,
      "loss": 1.4546,
      "step": 5600
    },
    {
      "epoch": 0.8100358422939068,
      "grad_norm": 96.83509063720703,
      "learning_rate": 1.7647520449126198e-05,
      "loss": 1.5103,
      "step": 5650
    },
    {
      "epoch": 0.8172043010752689,
      "grad_norm": 52.09833526611328,
      "learning_rate": 1.7617337237028766e-05,
      "loss": 1.6581,
      "step": 5700
    },
    {
      "epoch": 0.8243727598566308,
      "grad_norm": 68.51612091064453,
      "learning_rate": 1.7587154024931335e-05,
      "loss": 1.5091,
      "step": 5750
    },
    {
      "epoch": 0.8315412186379928,
      "grad_norm": 43.92729568481445,
      "learning_rate": 1.7556970812833903e-05,
      "loss": 1.5616,
      "step": 5800
    },
    {
      "epoch": 0.8387096774193549,
      "grad_norm": 50.5400505065918,
      "learning_rate": 1.7526787600736472e-05,
      "loss": 1.417,
      "step": 5850
    },
    {
      "epoch": 0.8458781362007168,
      "grad_norm": 41.58968734741211,
      "learning_rate": 1.749660438863904e-05,
      "loss": 1.6076,
      "step": 5900
    },
    {
      "epoch": 0.8530465949820788,
      "grad_norm": 134.29019165039062,
      "learning_rate": 1.746642117654161e-05,
      "loss": 1.5963,
      "step": 5950
    },
    {
      "epoch": 0.8602150537634409,
      "grad_norm": 85.39311218261719,
      "learning_rate": 1.7436237964444177e-05,
      "loss": 1.5891,
      "step": 6000
    },
    {
      "epoch": 0.8673835125448028,
      "grad_norm": 68.91819763183594,
      "learning_rate": 1.7406054752346746e-05,
      "loss": 1.5933,
      "step": 6050
    },
    {
      "epoch": 0.8745519713261649,
      "grad_norm": 136.87306213378906,
      "learning_rate": 1.7375871540249314e-05,
      "loss": 1.5421,
      "step": 6100
    },
    {
      "epoch": 0.8817204301075269,
      "grad_norm": 44.858192443847656,
      "learning_rate": 1.7345688328151883e-05,
      "loss": 1.4493,
      "step": 6150
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 76.70758819580078,
      "learning_rate": 1.731550511605445e-05,
      "loss": 1.5475,
      "step": 6200
    },
    {
      "epoch": 0.8960573476702509,
      "grad_norm": 150.49496459960938,
      "learning_rate": 1.728532190395702e-05,
      "loss": 1.5496,
      "step": 6250
    },
    {
      "epoch": 0.9032258064516129,
      "grad_norm": 116.65199279785156,
      "learning_rate": 1.725513869185959e-05,
      "loss": 1.4916,
      "step": 6300
    },
    {
      "epoch": 0.910394265232975,
      "grad_norm": 104.24698638916016,
      "learning_rate": 1.7224955479762157e-05,
      "loss": 1.571,
      "step": 6350
    },
    {
      "epoch": 0.9175627240143369,
      "grad_norm": 78.9054183959961,
      "learning_rate": 1.7195375931906675e-05,
      "loss": 1.5284,
      "step": 6400
    },
    {
      "epoch": 0.9247311827956989,
      "grad_norm": 106.78584289550781,
      "learning_rate": 1.7165192719809243e-05,
      "loss": 1.5845,
      "step": 6450
    },
    {
      "epoch": 0.931899641577061,
      "grad_norm": 60.75614547729492,
      "learning_rate": 1.7135009507711812e-05,
      "loss": 1.5817,
      "step": 6500
    },
    {
      "epoch": 0.9390681003584229,
      "grad_norm": 47.65946578979492,
      "learning_rate": 1.710482629561438e-05,
      "loss": 1.6222,
      "step": 6550
    },
    {
      "epoch": 0.946236559139785,
      "grad_norm": 48.58771896362305,
      "learning_rate": 1.707464308351695e-05,
      "loss": 1.5109,
      "step": 6600
    },
    {
      "epoch": 0.953405017921147,
      "grad_norm": 118.88809967041016,
      "learning_rate": 1.7044459871419517e-05,
      "loss": 1.4388,
      "step": 6650
    },
    {
      "epoch": 0.9605734767025089,
      "grad_norm": 39.9713249206543,
      "learning_rate": 1.7014276659322086e-05,
      "loss": 1.5587,
      "step": 6700
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 88.2680435180664,
      "learning_rate": 1.6984093447224655e-05,
      "loss": 1.6799,
      "step": 6750
    },
    {
      "epoch": 0.974910394265233,
      "grad_norm": 130.3756561279297,
      "learning_rate": 1.6953910235127223e-05,
      "loss": 1.5514,
      "step": 6800
    },
    {
      "epoch": 0.982078853046595,
      "grad_norm": 65.55506134033203,
      "learning_rate": 1.692372702302979e-05,
      "loss": 1.4668,
      "step": 6850
    },
    {
      "epoch": 0.989247311827957,
      "grad_norm": 59.22216033935547,
      "learning_rate": 1.689354381093236e-05,
      "loss": 1.5667,
      "step": 6900
    },
    {
      "epoch": 0.996415770609319,
      "grad_norm": 46.19839859008789,
      "learning_rate": 1.686336059883493e-05,
      "loss": 1.5624,
      "step": 6950
    },
    {
      "epoch": 1.003584229390681,
      "grad_norm": 103.88580322265625,
      "learning_rate": 1.6833177386737497e-05,
      "loss": 1.4835,
      "step": 7000
    },
    {
      "epoch": 1.010752688172043,
      "grad_norm": 60.89982223510742,
      "learning_rate": 1.6802994174640066e-05,
      "loss": 1.6211,
      "step": 7050
    },
    {
      "epoch": 1.017921146953405,
      "grad_norm": 26.76758575439453,
      "learning_rate": 1.6772810962542638e-05,
      "loss": 1.5226,
      "step": 7100
    },
    {
      "epoch": 1.025089605734767,
      "grad_norm": 38.08408737182617,
      "learning_rate": 1.6742627750445203e-05,
      "loss": 1.5114,
      "step": 7150
    },
    {
      "epoch": 1.032258064516129,
      "grad_norm": 82.14863586425781,
      "learning_rate": 1.671244453834777e-05,
      "loss": 1.5144,
      "step": 7200
    },
    {
      "epoch": 1.039426523297491,
      "grad_norm": 51.01852035522461,
      "learning_rate": 1.6682261326250343e-05,
      "loss": 1.5286,
      "step": 7250
    },
    {
      "epoch": 1.0465949820788532,
      "grad_norm": 70.28585815429688,
      "learning_rate": 1.6652078114152908e-05,
      "loss": 1.523,
      "step": 7300
    },
    {
      "epoch": 1.053763440860215,
      "grad_norm": 116.92529296875,
      "learning_rate": 1.6621894902055477e-05,
      "loss": 1.6291,
      "step": 7350
    },
    {
      "epoch": 1.060931899641577,
      "grad_norm": 154.8204803466797,
      "learning_rate": 1.659171168995805e-05,
      "loss": 1.4943,
      "step": 7400
    },
    {
      "epoch": 1.0681003584229392,
      "grad_norm": 37.76293182373047,
      "learning_rate": 1.6561528477860617e-05,
      "loss": 1.5578,
      "step": 7450
    },
    {
      "epoch": 1.075268817204301,
      "grad_norm": 68.72789764404297,
      "learning_rate": 1.6531345265763182e-05,
      "loss": 1.4962,
      "step": 7500
    },
    {
      "epoch": 1.082437275985663,
      "grad_norm": 37.386539459228516,
      "learning_rate": 1.6501162053665754e-05,
      "loss": 1.5473,
      "step": 7550
    },
    {
      "epoch": 1.0896057347670252,
      "grad_norm": 49.59426498413086,
      "learning_rate": 1.6470978841568323e-05,
      "loss": 1.4984,
      "step": 7600
    },
    {
      "epoch": 1.096774193548387,
      "grad_norm": 108.88502502441406,
      "learning_rate": 1.6440795629470888e-05,
      "loss": 1.5842,
      "step": 7650
    },
    {
      "epoch": 1.103942652329749,
      "grad_norm": 114.86710357666016,
      "learning_rate": 1.6410612417373457e-05,
      "loss": 1.5732,
      "step": 7700
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 44.28666687011719,
      "learning_rate": 1.638042920527603e-05,
      "loss": 1.589,
      "step": 7750
    },
    {
      "epoch": 1.118279569892473,
      "grad_norm": 52.58308792114258,
      "learning_rate": 1.6350245993178594e-05,
      "loss": 1.5328,
      "step": 7800
    },
    {
      "epoch": 1.125448028673835,
      "grad_norm": 99.17224884033203,
      "learning_rate": 1.6320062781081162e-05,
      "loss": 1.4704,
      "step": 7850
    },
    {
      "epoch": 1.1326164874551972,
      "grad_norm": 96.29908752441406,
      "learning_rate": 1.6289879568983734e-05,
      "loss": 1.5527,
      "step": 7900
    },
    {
      "epoch": 1.139784946236559,
      "grad_norm": 168.6223602294922,
      "learning_rate": 1.6259696356886303e-05,
      "loss": 1.5592,
      "step": 7950
    },
    {
      "epoch": 1.146953405017921,
      "grad_norm": 80.91637420654297,
      "learning_rate": 1.6229513144788868e-05,
      "loss": 1.5243,
      "step": 8000
    },
    {
      "epoch": 1.1541218637992832,
      "grad_norm": 158.60130310058594,
      "learning_rate": 1.619932993269144e-05,
      "loss": 1.4994,
      "step": 8050
    },
    {
      "epoch": 1.1612903225806452,
      "grad_norm": 28.900978088378906,
      "learning_rate": 1.6169146720594008e-05,
      "loss": 1.4736,
      "step": 8100
    },
    {
      "epoch": 1.168458781362007,
      "grad_norm": 149.56170654296875,
      "learning_rate": 1.6138963508496573e-05,
      "loss": 1.5209,
      "step": 8150
    },
    {
      "epoch": 1.1756272401433692,
      "grad_norm": 75.14207458496094,
      "learning_rate": 1.6108780296399145e-05,
      "loss": 1.5185,
      "step": 8200
    },
    {
      "epoch": 1.1827956989247312,
      "grad_norm": 219.6065216064453,
      "learning_rate": 1.6078597084301714e-05,
      "loss": 1.5099,
      "step": 8250
    },
    {
      "epoch": 1.189964157706093,
      "grad_norm": 47.33262252807617,
      "learning_rate": 1.604841387220428e-05,
      "loss": 1.5077,
      "step": 8300
    },
    {
      "epoch": 1.1971326164874552,
      "grad_norm": 135.7303009033203,
      "learning_rate": 1.601823066010685e-05,
      "loss": 1.489,
      "step": 8350
    },
    {
      "epoch": 1.2043010752688172,
      "grad_norm": 101.21601104736328,
      "learning_rate": 1.598804744800942e-05,
      "loss": 1.4408,
      "step": 8400
    },
    {
      "epoch": 1.2114695340501793,
      "grad_norm": 42.338436126708984,
      "learning_rate": 1.5957864235911988e-05,
      "loss": 1.4865,
      "step": 8450
    },
    {
      "epoch": 1.2186379928315412,
      "grad_norm": 47.71576690673828,
      "learning_rate": 1.5927681023814556e-05,
      "loss": 1.4143,
      "step": 8500
    },
    {
      "epoch": 1.2258064516129032,
      "grad_norm": 52.732154846191406,
      "learning_rate": 1.5898101475959074e-05,
      "loss": 1.5198,
      "step": 8550
    },
    {
      "epoch": 1.2329749103942653,
      "grad_norm": 95.99616241455078,
      "learning_rate": 1.5867918263861643e-05,
      "loss": 1.4248,
      "step": 8600
    },
    {
      "epoch": 1.2401433691756272,
      "grad_norm": 50.18438720703125,
      "learning_rate": 1.5837735051764208e-05,
      "loss": 1.552,
      "step": 8650
    },
    {
      "epoch": 1.2473118279569892,
      "grad_norm": 104.29383850097656,
      "learning_rate": 1.580755183966678e-05,
      "loss": 1.4393,
      "step": 8700
    },
    {
      "epoch": 1.2544802867383513,
      "grad_norm": 33.52751541137695,
      "learning_rate": 1.5777368627569348e-05,
      "loss": 1.5057,
      "step": 8750
    },
    {
      "epoch": 1.2616487455197132,
      "grad_norm": 50.45425033569336,
      "learning_rate": 1.5747185415471917e-05,
      "loss": 1.4935,
      "step": 8800
    },
    {
      "epoch": 1.2688172043010753,
      "grad_norm": 46.99325180053711,
      "learning_rate": 1.5717002203374485e-05,
      "loss": 1.5309,
      "step": 8850
    },
    {
      "epoch": 1.2759856630824373,
      "grad_norm": 59.82105255126953,
      "learning_rate": 1.5686818991277054e-05,
      "loss": 1.4769,
      "step": 8900
    },
    {
      "epoch": 1.2831541218637992,
      "grad_norm": 41.09029769897461,
      "learning_rate": 1.5656635779179622e-05,
      "loss": 1.4843,
      "step": 8950
    },
    {
      "epoch": 1.2903225806451613,
      "grad_norm": 34.844093322753906,
      "learning_rate": 1.562645256708219e-05,
      "loss": 1.5617,
      "step": 9000
    },
    {
      "epoch": 1.2974910394265233,
      "grad_norm": 84.11122131347656,
      "learning_rate": 1.559626935498476e-05,
      "loss": 1.4771,
      "step": 9050
    },
    {
      "epoch": 1.3046594982078852,
      "grad_norm": 37.84409713745117,
      "learning_rate": 1.5566086142887328e-05,
      "loss": 1.4894,
      "step": 9100
    },
    {
      "epoch": 1.3118279569892473,
      "grad_norm": 142.29200744628906,
      "learning_rate": 1.5535902930789896e-05,
      "loss": 1.4639,
      "step": 9150
    },
    {
      "epoch": 1.3189964157706093,
      "grad_norm": 94.52973937988281,
      "learning_rate": 1.5505719718692465e-05,
      "loss": 1.4623,
      "step": 9200
    },
    {
      "epoch": 1.3261648745519714,
      "grad_norm": 128.30914306640625,
      "learning_rate": 1.5475536506595033e-05,
      "loss": 1.6405,
      "step": 9250
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 117.87190246582031,
      "learning_rate": 1.5445353294497602e-05,
      "loss": 1.4261,
      "step": 9300
    },
    {
      "epoch": 1.3405017921146953,
      "grad_norm": 94.84635925292969,
      "learning_rate": 1.541517008240017e-05,
      "loss": 1.482,
      "step": 9350
    },
    {
      "epoch": 1.3476702508960574,
      "grad_norm": 120.51978302001953,
      "learning_rate": 1.538498687030274e-05,
      "loss": 1.601,
      "step": 9400
    },
    {
      "epoch": 1.3548387096774195,
      "grad_norm": 73.74388885498047,
      "learning_rate": 1.5354803658205308e-05,
      "loss": 1.5087,
      "step": 9450
    },
    {
      "epoch": 1.3620071684587813,
      "grad_norm": 56.90754699707031,
      "learning_rate": 1.5324620446107876e-05,
      "loss": 1.5018,
      "step": 9500
    },
    {
      "epoch": 1.3691756272401434,
      "grad_norm": 60.312347412109375,
      "learning_rate": 1.5294437234010445e-05,
      "loss": 1.541,
      "step": 9550
    },
    {
      "epoch": 1.3763440860215055,
      "grad_norm": 110.54373168945312,
      "learning_rate": 1.5264254021913013e-05,
      "loss": 1.3791,
      "step": 9600
    },
    {
      "epoch": 1.3835125448028673,
      "grad_norm": 52.47536087036133,
      "learning_rate": 1.5234070809815582e-05,
      "loss": 1.4797,
      "step": 9650
    },
    {
      "epoch": 1.3906810035842294,
      "grad_norm": 53.491851806640625,
      "learning_rate": 1.520388759771815e-05,
      "loss": 1.4345,
      "step": 9700
    },
    {
      "epoch": 1.3978494623655915,
      "grad_norm": 60.96763610839844,
      "learning_rate": 1.517370438562072e-05,
      "loss": 1.5399,
      "step": 9750
    },
    {
      "epoch": 1.4050179211469533,
      "grad_norm": 74.60835266113281,
      "learning_rate": 1.5143521173523287e-05,
      "loss": 1.5278,
      "step": 9800
    },
    {
      "epoch": 1.4121863799283154,
      "grad_norm": 41.164310455322266,
      "learning_rate": 1.5113337961425856e-05,
      "loss": 1.5167,
      "step": 9850
    },
    {
      "epoch": 1.4193548387096775,
      "grad_norm": 120.02440643310547,
      "learning_rate": 1.5083154749328426e-05,
      "loss": 1.415,
      "step": 9900
    },
    {
      "epoch": 1.4265232974910393,
      "grad_norm": 87.64510345458984,
      "learning_rate": 1.5052971537230994e-05,
      "loss": 1.6443,
      "step": 9950
    },
    {
      "epoch": 1.4336917562724014,
      "grad_norm": 78.70658111572266,
      "learning_rate": 1.5022788325133561e-05,
      "loss": 1.5165,
      "step": 10000
    },
    {
      "epoch": 1.4408602150537635,
      "grad_norm": 53.26738357543945,
      "learning_rate": 1.499260511303613e-05,
      "loss": 1.5823,
      "step": 10050
    },
    {
      "epoch": 1.4480286738351253,
      "grad_norm": 101.3155746459961,
      "learning_rate": 1.49624219009387e-05,
      "loss": 1.4634,
      "step": 10100
    },
    {
      "epoch": 1.4551971326164874,
      "grad_norm": 87.63397979736328,
      "learning_rate": 1.4932238688841267e-05,
      "loss": 1.4918,
      "step": 10150
    },
    {
      "epoch": 1.4623655913978495,
      "grad_norm": 82.1175765991211,
      "learning_rate": 1.4902055476743835e-05,
      "loss": 1.4462,
      "step": 10200
    },
    {
      "epoch": 1.4695340501792113,
      "grad_norm": 255.82289123535156,
      "learning_rate": 1.4871872264646406e-05,
      "loss": 1.5258,
      "step": 10250
    },
    {
      "epoch": 1.4767025089605734,
      "grad_norm": 44.35266876220703,
      "learning_rate": 1.4841689052548972e-05,
      "loss": 1.5028,
      "step": 10300
    },
    {
      "epoch": 1.4838709677419355,
      "grad_norm": 78.07976531982422,
      "learning_rate": 1.4811505840451541e-05,
      "loss": 1.4612,
      "step": 10350
    },
    {
      "epoch": 1.4910394265232976,
      "grad_norm": 55.724674224853516,
      "learning_rate": 1.4781322628354111e-05,
      "loss": 1.4529,
      "step": 10400
    },
    {
      "epoch": 1.4982078853046594,
      "grad_norm": 124.9554672241211,
      "learning_rate": 1.475113941625668e-05,
      "loss": 1.5303,
      "step": 10450
    },
    {
      "epoch": 1.5053763440860215,
      "grad_norm": 55.36139678955078,
      "learning_rate": 1.4720956204159247e-05,
      "loss": 1.4968,
      "step": 10500
    },
    {
      "epoch": 1.5125448028673834,
      "grad_norm": 179.84463500976562,
      "learning_rate": 1.4690772992061817e-05,
      "loss": 1.5371,
      "step": 10550
    },
    {
      "epoch": 1.5197132616487457,
      "grad_norm": 93.68927764892578,
      "learning_rate": 1.4661193444206335e-05,
      "loss": 1.5612,
      "step": 10600
    },
    {
      "epoch": 1.5268817204301075,
      "grad_norm": 68.56568908691406,
      "learning_rate": 1.4631010232108901e-05,
      "loss": 1.4827,
      "step": 10650
    },
    {
      "epoch": 1.5340501792114696,
      "grad_norm": 94.54997253417969,
      "learning_rate": 1.460082702001147e-05,
      "loss": 1.4552,
      "step": 10700
    },
    {
      "epoch": 1.5412186379928317,
      "grad_norm": 68.55359649658203,
      "learning_rate": 1.457064380791404e-05,
      "loss": 1.5071,
      "step": 10750
    },
    {
      "epoch": 1.5483870967741935,
      "grad_norm": 63.203678131103516,
      "learning_rate": 1.4540460595816609e-05,
      "loss": 1.4637,
      "step": 10800
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 39.28590774536133,
      "learning_rate": 1.4510277383719175e-05,
      "loss": 1.4839,
      "step": 10850
    },
    {
      "epoch": 1.5627240143369177,
      "grad_norm": 59.94184112548828,
      "learning_rate": 1.4480094171621746e-05,
      "loss": 1.5381,
      "step": 10900
    },
    {
      "epoch": 1.5698924731182795,
      "grad_norm": 54.84096908569336,
      "learning_rate": 1.4449910959524314e-05,
      "loss": 1.4981,
      "step": 10950
    },
    {
      "epoch": 1.5770609318996416,
      "grad_norm": 29.90532875061035,
      "learning_rate": 1.4419727747426881e-05,
      "loss": 1.4034,
      "step": 11000
    },
    {
      "epoch": 1.5842293906810037,
      "grad_norm": 63.23469543457031,
      "learning_rate": 1.4389544535329451e-05,
      "loss": 1.4769,
      "step": 11050
    },
    {
      "epoch": 1.5913978494623655,
      "grad_norm": 86.62483978271484,
      "learning_rate": 1.435936132323202e-05,
      "loss": 1.5484,
      "step": 11100
    },
    {
      "epoch": 1.5985663082437276,
      "grad_norm": 72.15178680419922,
      "learning_rate": 1.4329178111134587e-05,
      "loss": 1.4848,
      "step": 11150
    },
    {
      "epoch": 1.6057347670250897,
      "grad_norm": 140.89248657226562,
      "learning_rate": 1.4298994899037157e-05,
      "loss": 1.4043,
      "step": 11200
    },
    {
      "epoch": 1.6129032258064515,
      "grad_norm": 73.11458587646484,
      "learning_rate": 1.4268811686939725e-05,
      "loss": 1.3511,
      "step": 11250
    },
    {
      "epoch": 1.6200716845878136,
      "grad_norm": 81.74042510986328,
      "learning_rate": 1.4238628474842294e-05,
      "loss": 1.4537,
      "step": 11300
    },
    {
      "epoch": 1.6272401433691757,
      "grad_norm": 26.53163719177246,
      "learning_rate": 1.4208445262744862e-05,
      "loss": 1.4378,
      "step": 11350
    },
    {
      "epoch": 1.6344086021505375,
      "grad_norm": 56.086273193359375,
      "learning_rate": 1.4178262050647431e-05,
      "loss": 1.4681,
      "step": 11400
    },
    {
      "epoch": 1.6415770609318996,
      "grad_norm": 44.384334564208984,
      "learning_rate": 1.414807883855e-05,
      "loss": 1.4574,
      "step": 11450
    },
    {
      "epoch": 1.6487455197132617,
      "grad_norm": 23.343400955200195,
      "learning_rate": 1.411789562645257e-05,
      "loss": 1.4568,
      "step": 11500
    },
    {
      "epoch": 1.6559139784946235,
      "grad_norm": 95.39313507080078,
      "learning_rate": 1.4087712414355137e-05,
      "loss": 1.5504,
      "step": 11550
    },
    {
      "epoch": 1.6630824372759858,
      "grad_norm": 140.87954711914062,
      "learning_rate": 1.4057529202257705e-05,
      "loss": 1.5491,
      "step": 11600
    },
    {
      "epoch": 1.6702508960573477,
      "grad_norm": 167.1306610107422,
      "learning_rate": 1.4027345990160275e-05,
      "loss": 1.4045,
      "step": 11650
    },
    {
      "epoch": 1.6774193548387095,
      "grad_norm": 213.2017059326172,
      "learning_rate": 1.3997162778062842e-05,
      "loss": 1.4012,
      "step": 11700
    },
    {
      "epoch": 1.6845878136200718,
      "grad_norm": 55.07118225097656,
      "learning_rate": 1.396697956596541e-05,
      "loss": 1.4351,
      "step": 11750
    },
    {
      "epoch": 1.6917562724014337,
      "grad_norm": 52.074222564697266,
      "learning_rate": 1.393679635386798e-05,
      "loss": 1.4464,
      "step": 11800
    },
    {
      "epoch": 1.6989247311827957,
      "grad_norm": 122.38277435302734,
      "learning_rate": 1.390661314177055e-05,
      "loss": 1.4854,
      "step": 11850
    },
    {
      "epoch": 1.7060931899641578,
      "grad_norm": 46.43083953857422,
      "learning_rate": 1.3876429929673116e-05,
      "loss": 1.4464,
      "step": 11900
    },
    {
      "epoch": 1.7132616487455197,
      "grad_norm": 69.0225830078125,
      "learning_rate": 1.3846246717575686e-05,
      "loss": 1.4205,
      "step": 11950
    },
    {
      "epoch": 1.7204301075268817,
      "grad_norm": 39.09453201293945,
      "learning_rate": 1.3816063505478255e-05,
      "loss": 1.4483,
      "step": 12000
    },
    {
      "epoch": 1.7275985663082438,
      "grad_norm": 41.04753112792969,
      "learning_rate": 1.3785880293380822e-05,
      "loss": 1.4164,
      "step": 12050
    },
    {
      "epoch": 1.7347670250896057,
      "grad_norm": 41.55251693725586,
      "learning_rate": 1.3755697081283392e-05,
      "loss": 1.496,
      "step": 12100
    },
    {
      "epoch": 1.7419354838709677,
      "grad_norm": 59.118675231933594,
      "learning_rate": 1.372551386918596e-05,
      "loss": 1.4254,
      "step": 12150
    },
    {
      "epoch": 1.7491039426523298,
      "grad_norm": 56.720027923583984,
      "learning_rate": 1.3695330657088527e-05,
      "loss": 1.4303,
      "step": 12200
    },
    {
      "epoch": 1.7562724014336917,
      "grad_norm": 83.83139038085938,
      "learning_rate": 1.3665147444991098e-05,
      "loss": 1.5528,
      "step": 12250
    },
    {
      "epoch": 1.7634408602150538,
      "grad_norm": 70.981201171875,
      "learning_rate": 1.3634964232893666e-05,
      "loss": 1.4074,
      "step": 12300
    },
    {
      "epoch": 1.7706093189964158,
      "grad_norm": 60.70819091796875,
      "learning_rate": 1.3604781020796235e-05,
      "loss": 1.5184,
      "step": 12350
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 49.339149475097656,
      "learning_rate": 1.3574597808698803e-05,
      "loss": 1.5472,
      "step": 12400
    },
    {
      "epoch": 1.7849462365591398,
      "grad_norm": 55.4351806640625,
      "learning_rate": 1.3544414596601372e-05,
      "loss": 1.5292,
      "step": 12450
    },
    {
      "epoch": 1.7921146953405018,
      "grad_norm": 81.33860778808594,
      "learning_rate": 1.351423138450394e-05,
      "loss": 1.4718,
      "step": 12500
    },
    {
      "epoch": 1.7992831541218637,
      "grad_norm": 37.59245300292969,
      "learning_rate": 1.3484048172406507e-05,
      "loss": 1.4049,
      "step": 12550
    },
    {
      "epoch": 1.8064516129032258,
      "grad_norm": 140.53659057617188,
      "learning_rate": 1.3453864960309077e-05,
      "loss": 1.4645,
      "step": 12600
    },
    {
      "epoch": 1.8136200716845878,
      "grad_norm": 81.18183898925781,
      "learning_rate": 1.3424285412453595e-05,
      "loss": 1.4903,
      "step": 12650
    },
    {
      "epoch": 1.8207885304659497,
      "grad_norm": 213.79981994628906,
      "learning_rate": 1.3394102200356164e-05,
      "loss": 1.458,
      "step": 12700
    },
    {
      "epoch": 1.827956989247312,
      "grad_norm": 94.62178802490234,
      "learning_rate": 1.336391898825873e-05,
      "loss": 1.5073,
      "step": 12750
    },
    {
      "epoch": 1.8351254480286738,
      "grad_norm": 81.14270782470703,
      "learning_rate": 1.33337357761613e-05,
      "loss": 1.5471,
      "step": 12800
    },
    {
      "epoch": 1.8422939068100357,
      "grad_norm": 64.89045715332031,
      "learning_rate": 1.3303552564063869e-05,
      "loss": 1.4753,
      "step": 12850
    },
    {
      "epoch": 1.849462365591398,
      "grad_norm": 51.52842330932617,
      "learning_rate": 1.3273369351966436e-05,
      "loss": 1.4578,
      "step": 12900
    },
    {
      "epoch": 1.8566308243727598,
      "grad_norm": 56.6365852355957,
      "learning_rate": 1.3243186139869006e-05,
      "loss": 1.4555,
      "step": 12950
    },
    {
      "epoch": 1.863799283154122,
      "grad_norm": 33.43904113769531,
      "learning_rate": 1.3213002927771575e-05,
      "loss": 1.4767,
      "step": 13000
    },
    {
      "epoch": 1.870967741935484,
      "grad_norm": 74.93965148925781,
      "learning_rate": 1.3182819715674142e-05,
      "loss": 1.4203,
      "step": 13050
    },
    {
      "epoch": 1.8781362007168458,
      "grad_norm": 67.09234619140625,
      "learning_rate": 1.3152636503576712e-05,
      "loss": 1.4096,
      "step": 13100
    },
    {
      "epoch": 1.885304659498208,
      "grad_norm": 100.77851104736328,
      "learning_rate": 1.312245329147928e-05,
      "loss": 1.4275,
      "step": 13150
    },
    {
      "epoch": 1.89247311827957,
      "grad_norm": 33.40199661254883,
      "learning_rate": 1.3092270079381849e-05,
      "loss": 1.4961,
      "step": 13200
    },
    {
      "epoch": 1.8996415770609318,
      "grad_norm": 92.5815200805664,
      "learning_rate": 1.3062086867284417e-05,
      "loss": 1.5342,
      "step": 13250
    },
    {
      "epoch": 1.906810035842294,
      "grad_norm": 46.04892349243164,
      "learning_rate": 1.3031903655186986e-05,
      "loss": 1.4686,
      "step": 13300
    },
    {
      "epoch": 1.913978494623656,
      "grad_norm": 138.27313232421875,
      "learning_rate": 1.3001720443089554e-05,
      "loss": 1.4384,
      "step": 13350
    },
    {
      "epoch": 1.9211469534050178,
      "grad_norm": 74.4681167602539,
      "learning_rate": 1.2971537230992125e-05,
      "loss": 1.529,
      "step": 13400
    },
    {
      "epoch": 1.92831541218638,
      "grad_norm": 91.0648193359375,
      "learning_rate": 1.2941354018894691e-05,
      "loss": 1.4044,
      "step": 13450
    },
    {
      "epoch": 1.935483870967742,
      "grad_norm": 43.870811462402344,
      "learning_rate": 1.291117080679726e-05,
      "loss": 1.4619,
      "step": 13500
    },
    {
      "epoch": 1.9426523297491038,
      "grad_norm": 66.37284851074219,
      "learning_rate": 1.288098759469983e-05,
      "loss": 1.4771,
      "step": 13550
    },
    {
      "epoch": 1.949820788530466,
      "grad_norm": 59.23664855957031,
      "learning_rate": 1.2850804382602397e-05,
      "loss": 1.5329,
      "step": 13600
    },
    {
      "epoch": 1.956989247311828,
      "grad_norm": 159.31455993652344,
      "learning_rate": 1.2820621170504966e-05,
      "loss": 1.4054,
      "step": 13650
    },
    {
      "epoch": 1.9641577060931898,
      "grad_norm": 73.12272644042969,
      "learning_rate": 1.2790437958407536e-05,
      "loss": 1.4898,
      "step": 13700
    },
    {
      "epoch": 1.971326164874552,
      "grad_norm": 48.770320892333984,
      "learning_rate": 1.2760254746310104e-05,
      "loss": 1.4906,
      "step": 13750
    },
    {
      "epoch": 1.978494623655914,
      "grad_norm": 29.840179443359375,
      "learning_rate": 1.2730071534212671e-05,
      "loss": 1.5635,
      "step": 13800
    },
    {
      "epoch": 1.9856630824372759,
      "grad_norm": 57.19105911254883,
      "learning_rate": 1.2699888322115241e-05,
      "loss": 1.4983,
      "step": 13850
    },
    {
      "epoch": 1.9928315412186381,
      "grad_norm": 58.423683166503906,
      "learning_rate": 1.266970511001781e-05,
      "loss": 1.4589,
      "step": 13900
    },
    {
      "epoch": 2.0,
      "grad_norm": 200.73870849609375,
      "learning_rate": 1.2639521897920377e-05,
      "loss": 1.444,
      "step": 13950
    },
    {
      "epoch": 2.007168458781362,
      "grad_norm": 51.38685607910156,
      "learning_rate": 1.2609338685822947e-05,
      "loss": 1.5697,
      "step": 14000
    },
    {
      "epoch": 2.014336917562724,
      "grad_norm": 46.726749420166016,
      "learning_rate": 1.2579155473725515e-05,
      "loss": 1.3644,
      "step": 14050
    },
    {
      "epoch": 2.021505376344086,
      "grad_norm": 57.98714828491211,
      "learning_rate": 1.2548972261628082e-05,
      "loss": 1.4704,
      "step": 14100
    },
    {
      "epoch": 2.028673835125448,
      "grad_norm": 55.031883239746094,
      "learning_rate": 1.2518789049530652e-05,
      "loss": 1.4781,
      "step": 14150
    },
    {
      "epoch": 2.03584229390681,
      "grad_norm": 99.94676208496094,
      "learning_rate": 1.2488605837433221e-05,
      "loss": 1.4255,
      "step": 14200
    },
    {
      "epoch": 2.043010752688172,
      "grad_norm": 146.72979736328125,
      "learning_rate": 1.245842262533579e-05,
      "loss": 1.4824,
      "step": 14250
    },
    {
      "epoch": 2.050179211469534,
      "grad_norm": 216.30740356445312,
      "learning_rate": 1.2428239413238358e-05,
      "loss": 1.4047,
      "step": 14300
    },
    {
      "epoch": 2.057347670250896,
      "grad_norm": 38.197723388671875,
      "learning_rate": 1.2398056201140927e-05,
      "loss": 1.4254,
      "step": 14350
    },
    {
      "epoch": 2.064516129032258,
      "grad_norm": 87.27371215820312,
      "learning_rate": 1.2367872989043495e-05,
      "loss": 1.4478,
      "step": 14400
    },
    {
      "epoch": 2.07168458781362,
      "grad_norm": 69.77389526367188,
      "learning_rate": 1.2337689776946065e-05,
      "loss": 1.4656,
      "step": 14450
    },
    {
      "epoch": 2.078853046594982,
      "grad_norm": 118.99552917480469,
      "learning_rate": 1.2307506564848632e-05,
      "loss": 1.4216,
      "step": 14500
    },
    {
      "epoch": 2.086021505376344,
      "grad_norm": 59.98612976074219,
      "learning_rate": 1.22773233527512e-05,
      "loss": 1.4814,
      "step": 14550
    },
    {
      "epoch": 2.0931899641577063,
      "grad_norm": 66.01204681396484,
      "learning_rate": 1.2247140140653771e-05,
      "loss": 1.4456,
      "step": 14600
    },
    {
      "epoch": 2.100358422939068,
      "grad_norm": 43.630775451660156,
      "learning_rate": 1.2217560592798287e-05,
      "loss": 1.4076,
      "step": 14650
    },
    {
      "epoch": 2.10752688172043,
      "grad_norm": 42.213321685791016,
      "learning_rate": 1.2187377380700855e-05,
      "loss": 1.3738,
      "step": 14700
    },
    {
      "epoch": 2.1146953405017923,
      "grad_norm": 50.350040435791016,
      "learning_rate": 1.2157194168603424e-05,
      "loss": 1.5291,
      "step": 14750
    },
    {
      "epoch": 2.121863799283154,
      "grad_norm": 161.28358459472656,
      "learning_rate": 1.212701095650599e-05,
      "loss": 1.43,
      "step": 14800
    },
    {
      "epoch": 2.129032258064516,
      "grad_norm": 65.77596282958984,
      "learning_rate": 1.2096827744408561e-05,
      "loss": 1.4384,
      "step": 14850
    },
    {
      "epoch": 2.1362007168458783,
      "grad_norm": 54.12208557128906,
      "learning_rate": 1.206664453231113e-05,
      "loss": 1.4601,
      "step": 14900
    },
    {
      "epoch": 2.14336917562724,
      "grad_norm": 38.48611831665039,
      "learning_rate": 1.2036461320213696e-05,
      "loss": 1.4141,
      "step": 14950
    },
    {
      "epoch": 2.150537634408602,
      "grad_norm": 73.76634979248047,
      "learning_rate": 1.2006278108116267e-05,
      "loss": 1.4672,
      "step": 15000
    },
    {
      "epoch": 2.1577060931899643,
      "grad_norm": 117.64169311523438,
      "learning_rate": 1.1976094896018835e-05,
      "loss": 1.3667,
      "step": 15050
    },
    {
      "epoch": 2.164874551971326,
      "grad_norm": 173.2177734375,
      "learning_rate": 1.1945911683921404e-05,
      "loss": 1.4857,
      "step": 15100
    },
    {
      "epoch": 2.172043010752688,
      "grad_norm": 74.12728881835938,
      "learning_rate": 1.1915728471823972e-05,
      "loss": 1.4644,
      "step": 15150
    },
    {
      "epoch": 2.1792114695340503,
      "grad_norm": 142.28463745117188,
      "learning_rate": 1.188554525972654e-05,
      "loss": 1.4315,
      "step": 15200
    },
    {
      "epoch": 2.186379928315412,
      "grad_norm": 82.18484497070312,
      "learning_rate": 1.185536204762911e-05,
      "loss": 1.4859,
      "step": 15250
    },
    {
      "epoch": 2.193548387096774,
      "grad_norm": 48.790706634521484,
      "learning_rate": 1.182517883553168e-05,
      "loss": 1.4689,
      "step": 15300
    },
    {
      "epoch": 2.2007168458781363,
      "grad_norm": 48.3781852722168,
      "learning_rate": 1.1794995623434246e-05,
      "loss": 1.4783,
      "step": 15350
    },
    {
      "epoch": 2.207885304659498,
      "grad_norm": 141.44656372070312,
      "learning_rate": 1.1764812411336815e-05,
      "loss": 1.4276,
      "step": 15400
    },
    {
      "epoch": 2.21505376344086,
      "grad_norm": 111.82347106933594,
      "learning_rate": 1.1734629199239385e-05,
      "loss": 1.4513,
      "step": 15450
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 44.829750061035156,
      "learning_rate": 1.1704445987141952e-05,
      "loss": 1.4273,
      "step": 15500
    },
    {
      "epoch": 2.229390681003584,
      "grad_norm": 90.65984344482422,
      "learning_rate": 1.167426277504452e-05,
      "loss": 1.462,
      "step": 15550
    },
    {
      "epoch": 2.236559139784946,
      "grad_norm": 48.96239471435547,
      "learning_rate": 1.164407956294709e-05,
      "loss": 1.4897,
      "step": 15600
    },
    {
      "epoch": 2.2437275985663083,
      "grad_norm": 55.033748626708984,
      "learning_rate": 1.1613896350849657e-05,
      "loss": 1.4974,
      "step": 15650
    },
    {
      "epoch": 2.25089605734767,
      "grad_norm": 55.03618621826172,
      "learning_rate": 1.1583713138752226e-05,
      "loss": 1.5416,
      "step": 15700
    },
    {
      "epoch": 2.258064516129032,
      "grad_norm": 135.84243774414062,
      "learning_rate": 1.1553529926654796e-05,
      "loss": 1.4094,
      "step": 15750
    },
    {
      "epoch": 2.2652329749103943,
      "grad_norm": 64.17169189453125,
      "learning_rate": 1.1523346714557365e-05,
      "loss": 1.492,
      "step": 15800
    },
    {
      "epoch": 2.272401433691756,
      "grad_norm": 68.25051879882812,
      "learning_rate": 1.1493163502459932e-05,
      "loss": 1.5405,
      "step": 15850
    },
    {
      "epoch": 2.279569892473118,
      "grad_norm": 71.24476623535156,
      "learning_rate": 1.1462980290362502e-05,
      "loss": 1.5371,
      "step": 15900
    },
    {
      "epoch": 2.2867383512544803,
      "grad_norm": 145.11367797851562,
      "learning_rate": 1.143279707826507e-05,
      "loss": 1.4078,
      "step": 15950
    },
    {
      "epoch": 2.293906810035842,
      "grad_norm": 72.09644317626953,
      "learning_rate": 1.1402613866167637e-05,
      "loss": 1.4427,
      "step": 16000
    },
    {
      "epoch": 2.3010752688172045,
      "grad_norm": 56.933860778808594,
      "learning_rate": 1.1372430654070207e-05,
      "loss": 1.4163,
      "step": 16050
    },
    {
      "epoch": 2.3082437275985663,
      "grad_norm": 55.33739471435547,
      "learning_rate": 1.1342247441972776e-05,
      "loss": 1.3554,
      "step": 16100
    },
    {
      "epoch": 2.315412186379928,
      "grad_norm": 58.61513900756836,
      "learning_rate": 1.1312064229875344e-05,
      "loss": 1.4876,
      "step": 16150
    },
    {
      "epoch": 2.3225806451612905,
      "grad_norm": 57.769927978515625,
      "learning_rate": 1.1281881017777913e-05,
      "loss": 1.3975,
      "step": 16200
    },
    {
      "epoch": 2.3297491039426523,
      "grad_norm": 56.72529983520508,
      "learning_rate": 1.1251697805680481e-05,
      "loss": 1.3697,
      "step": 16250
    },
    {
      "epoch": 2.336917562724014,
      "grad_norm": 25.747377395629883,
      "learning_rate": 1.122151459358305e-05,
      "loss": 1.4548,
      "step": 16300
    },
    {
      "epoch": 2.3440860215053765,
      "grad_norm": 56.72254180908203,
      "learning_rate": 1.119133138148562e-05,
      "loss": 1.36,
      "step": 16350
    },
    {
      "epoch": 2.3512544802867383,
      "grad_norm": 42.49698257446289,
      "learning_rate": 1.1161148169388187e-05,
      "loss": 1.3938,
      "step": 16400
    },
    {
      "epoch": 2.3584229390681,
      "grad_norm": 195.86402893066406,
      "learning_rate": 1.1130964957290756e-05,
      "loss": 1.4208,
      "step": 16450
    },
    {
      "epoch": 2.3655913978494625,
      "grad_norm": 171.91497802734375,
      "learning_rate": 1.1100781745193326e-05,
      "loss": 1.3574,
      "step": 16500
    },
    {
      "epoch": 2.3727598566308243,
      "grad_norm": 77.93275451660156,
      "learning_rate": 1.1070598533095893e-05,
      "loss": 1.5242,
      "step": 16550
    },
    {
      "epoch": 2.379928315412186,
      "grad_norm": 83.79436492919922,
      "learning_rate": 1.1040415320998461e-05,
      "loss": 1.4117,
      "step": 16600
    },
    {
      "epoch": 2.3870967741935485,
      "grad_norm": 50.70182418823242,
      "learning_rate": 1.1010232108901031e-05,
      "loss": 1.4212,
      "step": 16650
    },
    {
      "epoch": 2.3942652329749103,
      "grad_norm": 100.6781997680664,
      "learning_rate": 1.0980048896803598e-05,
      "loss": 1.4228,
      "step": 16700
    },
    {
      "epoch": 2.4014336917562726,
      "grad_norm": 29.931488037109375,
      "learning_rate": 1.0950469348948116e-05,
      "loss": 1.4306,
      "step": 16750
    },
    {
      "epoch": 2.4086021505376345,
      "grad_norm": 49.02875900268555,
      "learning_rate": 1.0920286136850684e-05,
      "loss": 1.4983,
      "step": 16800
    },
    {
      "epoch": 2.4157706093189963,
      "grad_norm": 96.01663970947266,
      "learning_rate": 1.0890102924753255e-05,
      "loss": 1.4354,
      "step": 16850
    },
    {
      "epoch": 2.4229390681003586,
      "grad_norm": 42.506351470947266,
      "learning_rate": 1.0859919712655822e-05,
      "loss": 1.4007,
      "step": 16900
    },
    {
      "epoch": 2.4301075268817205,
      "grad_norm": 40.54417419433594,
      "learning_rate": 1.082973650055839e-05,
      "loss": 1.4644,
      "step": 16950
    },
    {
      "epoch": 2.4372759856630823,
      "grad_norm": 48.90232467651367,
      "learning_rate": 1.079955328846096e-05,
      "loss": 1.4385,
      "step": 17000
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 45.5202522277832,
      "learning_rate": 1.0769370076363527e-05,
      "loss": 1.4856,
      "step": 17050
    },
    {
      "epoch": 2.4516129032258065,
      "grad_norm": 67.4811019897461,
      "learning_rate": 1.0739186864266096e-05,
      "loss": 1.5059,
      "step": 17100
    },
    {
      "epoch": 2.4587813620071683,
      "grad_norm": 129.92942810058594,
      "learning_rate": 1.0709003652168664e-05,
      "loss": 1.3553,
      "step": 17150
    },
    {
      "epoch": 2.4659498207885306,
      "grad_norm": 38.54247283935547,
      "learning_rate": 1.0678820440071234e-05,
      "loss": 1.5432,
      "step": 17200
    },
    {
      "epoch": 2.4731182795698925,
      "grad_norm": 56.63740539550781,
      "learning_rate": 1.0648637227973801e-05,
      "loss": 1.404,
      "step": 17250
    },
    {
      "epoch": 2.4802867383512543,
      "grad_norm": 38.0750617980957,
      "learning_rate": 1.061845401587637e-05,
      "loss": 1.4431,
      "step": 17300
    },
    {
      "epoch": 2.4874551971326166,
      "grad_norm": 174.775634765625,
      "learning_rate": 1.058827080377894e-05,
      "loss": 1.432,
      "step": 17350
    },
    {
      "epoch": 2.4946236559139785,
      "grad_norm": 47.86268615722656,
      "learning_rate": 1.0558087591681507e-05,
      "loss": 1.4908,
      "step": 17400
    },
    {
      "epoch": 2.5017921146953404,
      "grad_norm": 54.83498764038086,
      "learning_rate": 1.0527904379584075e-05,
      "loss": 1.4384,
      "step": 17450
    },
    {
      "epoch": 2.5089605734767026,
      "grad_norm": 111.2319107055664,
      "learning_rate": 1.0497721167486646e-05,
      "loss": 1.4971,
      "step": 17500
    },
    {
      "epoch": 2.5161290322580645,
      "grad_norm": 86.56820678710938,
      "learning_rate": 1.0467537955389212e-05,
      "loss": 1.3729,
      "step": 17550
    },
    {
      "epoch": 2.5232974910394264,
      "grad_norm": 48.40281295776367,
      "learning_rate": 1.0437354743291781e-05,
      "loss": 1.3625,
      "step": 17600
    },
    {
      "epoch": 2.5304659498207887,
      "grad_norm": 32.22092819213867,
      "learning_rate": 1.0407171531194351e-05,
      "loss": 1.3954,
      "step": 17650
    },
    {
      "epoch": 2.5376344086021505,
      "grad_norm": 49.61455535888672,
      "learning_rate": 1.037698831909692e-05,
      "loss": 1.4517,
      "step": 17700
    },
    {
      "epoch": 2.5448028673835124,
      "grad_norm": 110.30586242675781,
      "learning_rate": 1.0346805106999486e-05,
      "loss": 1.4768,
      "step": 17750
    },
    {
      "epoch": 2.5519713261648747,
      "grad_norm": 85.46484375,
      "learning_rate": 1.0316621894902057e-05,
      "loss": 1.4488,
      "step": 17800
    },
    {
      "epoch": 2.5591397849462365,
      "grad_norm": 107.00691986083984,
      "learning_rate": 1.0286438682804625e-05,
      "loss": 1.4329,
      "step": 17850
    },
    {
      "epoch": 2.5663082437275984,
      "grad_norm": 87.66896057128906,
      "learning_rate": 1.0256255470707192e-05,
      "loss": 1.4809,
      "step": 17900
    },
    {
      "epoch": 2.5734767025089607,
      "grad_norm": 83.08363342285156,
      "learning_rate": 1.0226072258609762e-05,
      "loss": 1.4147,
      "step": 17950
    },
    {
      "epoch": 2.5806451612903225,
      "grad_norm": 83.82027435302734,
      "learning_rate": 1.019588904651233e-05,
      "loss": 1.3395,
      "step": 18000
    },
    {
      "epoch": 2.5878136200716844,
      "grad_norm": 118.16085815429688,
      "learning_rate": 1.0165705834414898e-05,
      "loss": 1.4535,
      "step": 18050
    },
    {
      "epoch": 2.5949820788530467,
      "grad_norm": 56.1170539855957,
      "learning_rate": 1.0135522622317468e-05,
      "loss": 1.3935,
      "step": 18100
    },
    {
      "epoch": 2.6021505376344085,
      "grad_norm": 95.5599136352539,
      "learning_rate": 1.0105339410220036e-05,
      "loss": 1.4513,
      "step": 18150
    },
    {
      "epoch": 2.6093189964157704,
      "grad_norm": 71.10405731201172,
      "learning_rate": 1.0075156198122605e-05,
      "loss": 1.3923,
      "step": 18200
    },
    {
      "epoch": 2.6164874551971327,
      "grad_norm": 55.56751251220703,
      "learning_rate": 1.0044972986025175e-05,
      "loss": 1.3866,
      "step": 18250
    },
    {
      "epoch": 2.6236559139784945,
      "grad_norm": 25.107362747192383,
      "learning_rate": 1.0014789773927742e-05,
      "loss": 1.5132,
      "step": 18300
    },
    {
      "epoch": 2.6308243727598564,
      "grad_norm": 297.4141845703125,
      "learning_rate": 9.98460656183031e-06,
      "loss": 1.5051,
      "step": 18350
    },
    {
      "epoch": 2.6379928315412187,
      "grad_norm": 209.76927185058594,
      "learning_rate": 9.954423349732879e-06,
      "loss": 1.3453,
      "step": 18400
    },
    {
      "epoch": 2.6451612903225805,
      "grad_norm": 110.78933715820312,
      "learning_rate": 9.924240137635448e-06,
      "loss": 1.4042,
      "step": 18450
    },
    {
      "epoch": 2.652329749103943,
      "grad_norm": 62.08601379394531,
      "learning_rate": 9.894056925538018e-06,
      "loss": 1.4179,
      "step": 18500
    },
    {
      "epoch": 2.6594982078853047,
      "grad_norm": 85.82827758789062,
      "learning_rate": 9.863873713440585e-06,
      "loss": 1.4195,
      "step": 18550
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 50.95008850097656,
      "learning_rate": 9.833690501343153e-06,
      "loss": 1.4178,
      "step": 18600
    },
    {
      "epoch": 2.673835125448029,
      "grad_norm": 100.85750579833984,
      "learning_rate": 9.803507289245723e-06,
      "loss": 1.4945,
      "step": 18650
    },
    {
      "epoch": 2.6810035842293907,
      "grad_norm": 176.8050079345703,
      "learning_rate": 9.77332407714829e-06,
      "loss": 1.457,
      "step": 18700
    },
    {
      "epoch": 2.688172043010753,
      "grad_norm": 96.22457885742188,
      "learning_rate": 9.74314086505086e-06,
      "loss": 1.4018,
      "step": 18750
    },
    {
      "epoch": 2.695340501792115,
      "grad_norm": 100.2419204711914,
      "learning_rate": 9.713561317195376e-06,
      "loss": 1.4254,
      "step": 18800
    },
    {
      "epoch": 2.7025089605734767,
      "grad_norm": 63.75444793701172,
      "learning_rate": 9.683378105097947e-06,
      "loss": 1.4437,
      "step": 18850
    },
    {
      "epoch": 2.709677419354839,
      "grad_norm": 76.13230895996094,
      "learning_rate": 9.653194893000513e-06,
      "loss": 1.4439,
      "step": 18900
    },
    {
      "epoch": 2.716845878136201,
      "grad_norm": 30.95268440246582,
      "learning_rate": 9.623011680903082e-06,
      "loss": 1.4105,
      "step": 18950
    },
    {
      "epoch": 2.7240143369175627,
      "grad_norm": 74.59169006347656,
      "learning_rate": 9.59282846880565e-06,
      "loss": 1.3743,
      "step": 19000
    },
    {
      "epoch": 2.731182795698925,
      "grad_norm": 33.84809875488281,
      "learning_rate": 9.562645256708219e-06,
      "loss": 1.3712,
      "step": 19050
    },
    {
      "epoch": 2.738351254480287,
      "grad_norm": 48.98439025878906,
      "learning_rate": 9.53246204461079e-06,
      "loss": 1.4761,
      "step": 19100
    },
    {
      "epoch": 2.7455197132616487,
      "grad_norm": 50.41459274291992,
      "learning_rate": 9.502278832513356e-06,
      "loss": 1.3779,
      "step": 19150
    },
    {
      "epoch": 2.752688172043011,
      "grad_norm": 57.685523986816406,
      "learning_rate": 9.472095620415925e-06,
      "loss": 1.4306,
      "step": 19200
    },
    {
      "epoch": 2.759856630824373,
      "grad_norm": 70.11664581298828,
      "learning_rate": 9.441912408318495e-06,
      "loss": 1.4358,
      "step": 19250
    },
    {
      "epoch": 2.7670250896057347,
      "grad_norm": 108.6549301147461,
      "learning_rate": 9.411729196221062e-06,
      "loss": 1.438,
      "step": 19300
    },
    {
      "epoch": 2.774193548387097,
      "grad_norm": 169.18759155273438,
      "learning_rate": 9.381545984123632e-06,
      "loss": 1.4256,
      "step": 19350
    },
    {
      "epoch": 2.781362007168459,
      "grad_norm": 68.73197937011719,
      "learning_rate": 9.3513627720262e-06,
      "loss": 1.4453,
      "step": 19400
    },
    {
      "epoch": 2.7885304659498207,
      "grad_norm": 38.190975189208984,
      "learning_rate": 9.321179559928767e-06,
      "loss": 1.4998,
      "step": 19450
    },
    {
      "epoch": 2.795698924731183,
      "grad_norm": 47.21006774902344,
      "learning_rate": 9.290996347831337e-06,
      "loss": 1.4997,
      "step": 19500
    },
    {
      "epoch": 2.802867383512545,
      "grad_norm": 47.87800598144531,
      "learning_rate": 9.260813135733906e-06,
      "loss": 1.3732,
      "step": 19550
    },
    {
      "epoch": 2.8100358422939067,
      "grad_norm": 52.48378372192383,
      "learning_rate": 9.230629923636475e-06,
      "loss": 1.4383,
      "step": 19600
    },
    {
      "epoch": 2.817204301075269,
      "grad_norm": 51.08073043823242,
      "learning_rate": 9.200446711539043e-06,
      "loss": 1.4713,
      "step": 19650
    },
    {
      "epoch": 2.824372759856631,
      "grad_norm": 57.5313720703125,
      "learning_rate": 9.170263499441612e-06,
      "loss": 1.4127,
      "step": 19700
    },
    {
      "epoch": 2.8315412186379927,
      "grad_norm": 127.87147521972656,
      "learning_rate": 9.14008028734418e-06,
      "loss": 1.3399,
      "step": 19750
    },
    {
      "epoch": 2.838709677419355,
      "grad_norm": 31.640111923217773,
      "learning_rate": 9.109897075246749e-06,
      "loss": 1.3516,
      "step": 19800
    },
    {
      "epoch": 2.845878136200717,
      "grad_norm": 41.61192321777344,
      "learning_rate": 9.079713863149317e-06,
      "loss": 1.3716,
      "step": 19850
    },
    {
      "epoch": 2.8530465949820787,
      "grad_norm": 77.93460083007812,
      "learning_rate": 9.049530651051886e-06,
      "loss": 1.4379,
      "step": 19900
    },
    {
      "epoch": 2.860215053763441,
      "grad_norm": 155.69125366210938,
      "learning_rate": 9.019347438954454e-06,
      "loss": 1.3656,
      "step": 19950
    },
    {
      "epoch": 2.867383512544803,
      "grad_norm": 48.34747314453125,
      "learning_rate": 8.989164226857023e-06,
      "loss": 1.4067,
      "step": 20000
    },
    {
      "epoch": 2.8745519713261647,
      "grad_norm": 188.52044677734375,
      "learning_rate": 8.958981014759591e-06,
      "loss": 1.3955,
      "step": 20050
    },
    {
      "epoch": 2.881720430107527,
      "grad_norm": 142.73976135253906,
      "learning_rate": 8.92879780266216e-06,
      "loss": 1.3313,
      "step": 20100
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 103.20484161376953,
      "learning_rate": 8.898614590564728e-06,
      "loss": 1.4269,
      "step": 20150
    },
    {
      "epoch": 2.8960573476702507,
      "grad_norm": 113.05880737304688,
      "learning_rate": 8.868431378467297e-06,
      "loss": 1.4355,
      "step": 20200
    },
    {
      "epoch": 2.903225806451613,
      "grad_norm": 36.66958999633789,
      "learning_rate": 8.838248166369865e-06,
      "loss": 1.402,
      "step": 20250
    },
    {
      "epoch": 2.910394265232975,
      "grad_norm": 97.6771240234375,
      "learning_rate": 8.808064954272434e-06,
      "loss": 1.4869,
      "step": 20300
    },
    {
      "epoch": 2.9175627240143367,
      "grad_norm": 186.60511779785156,
      "learning_rate": 8.777881742175002e-06,
      "loss": 1.3528,
      "step": 20350
    },
    {
      "epoch": 2.924731182795699,
      "grad_norm": 95.3625717163086,
      "learning_rate": 8.747698530077573e-06,
      "loss": 1.4328,
      "step": 20400
    },
    {
      "epoch": 2.931899641577061,
      "grad_norm": 58.8077392578125,
      "learning_rate": 8.71751531798014e-06,
      "loss": 1.4708,
      "step": 20450
    },
    {
      "epoch": 2.9390681003584227,
      "grad_norm": 90.22132873535156,
      "learning_rate": 8.687332105882708e-06,
      "loss": 1.51,
      "step": 20500
    },
    {
      "epoch": 2.946236559139785,
      "grad_norm": 82.15701293945312,
      "learning_rate": 8.657148893785278e-06,
      "loss": 1.3697,
      "step": 20550
    },
    {
      "epoch": 2.953405017921147,
      "grad_norm": 121.41486358642578,
      "learning_rate": 8.626965681687845e-06,
      "loss": 1.4455,
      "step": 20600
    },
    {
      "epoch": 2.9605734767025087,
      "grad_norm": 87.64840698242188,
      "learning_rate": 8.596782469590415e-06,
      "loss": 1.4184,
      "step": 20650
    },
    {
      "epoch": 2.967741935483871,
      "grad_norm": 127.11383819580078,
      "learning_rate": 8.566599257492984e-06,
      "loss": 1.4077,
      "step": 20700
    },
    {
      "epoch": 2.974910394265233,
      "grad_norm": 55.86671447753906,
      "learning_rate": 8.53641604539555e-06,
      "loss": 1.4408,
      "step": 20750
    },
    {
      "epoch": 2.982078853046595,
      "grad_norm": 71.47960662841797,
      "learning_rate": 8.506836497540068e-06,
      "loss": 1.3749,
      "step": 20800
    },
    {
      "epoch": 2.989247311827957,
      "grad_norm": 33.70003128051758,
      "learning_rate": 8.476653285442637e-06,
      "loss": 1.3981,
      "step": 20850
    },
    {
      "epoch": 2.996415770609319,
      "grad_norm": 26.21369743347168,
      "learning_rate": 8.446470073345207e-06,
      "loss": 1.3893,
      "step": 20900
    }
  ],
  "logging_steps": 50,
  "max_steps": 34875,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.5562830354466734e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
